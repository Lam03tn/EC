{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9784981,"sourceType":"datasetVersion","datasetId":5994771},{"sourceId":9925047,"sourceType":"datasetVersion","datasetId":6100243},{"sourceId":10117413,"sourceType":"datasetVersion","datasetId":6242360}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nimport pandas as pd\nimport os\n\n# Đường dẫn đến thư mục chứa các file CSV\nfolder_path = '/kaggle/input/data-high-rate/hour'\n\n# Danh sách chứa tất cả các DataFrame\nall_dataframes = []\n\n# Lặp qua tất cả các file CSV trong thư mục\n# for file_name in os.listdir(folder_path):\n#     if file_name.endswith('.csv'):\n#         file_path = os.path.join(folder_path, file_name)\n#         df = pd.read_csv(file_path)\n#         all_dataframes.append(df)\n\n# daily\nfor folder in os.listdir(folder_path):\n    if folder == \"merge\":\n        continue\n    for file_name in os.listdir(os.path.join(folder_path,folder)):\n        if file_name.endswith('.csv'):\n            file_path = os.path.join(folder_path, folder,file_name)\n            df = pd.read_csv(file_path)\n            all_dataframes.append(df)\n# print(all_dataframes)\n\n# Gộp tất cả các DataFrame lại\nmerged_df = pd.concat(all_dataframes, ignore_index=True)\n# Lưu DataFrame gộp vào một file CSV mới\nmerged_df.to_csv('merged_file.csv', index=False)\n\n# Đọc dữ liệu từ tệp CSV\nfile_path = \"/kaggle/working/merged_file.csv\"\ndf = pd.read_csv(file_path)\n\n# Lựa chọn các đặc trưng: thời gian, nhiệt độ, kinh độ, vĩ độ\n# Chuyển đổi cột thời gian thành số (giả sử là số giờ từ đầu ngày)\ndf['time'] = pd.to_datetime(df['time'])\ndf['hour'] = df['time'].dt.hour\n\n# Chuyển đổi cột thời gian thành định dạng datetime\ndf['time'] = pd.to_datetime(df['time'])\n\n# Chọn một ngày mốc\nepoch = pd.Timestamp('2022-01-01')\n\n# Tính toán số ngày từ ngày mốc và cộng thêm số giờ\ndf['time_numeric'] = (df['time'] - epoch).dt.days + df['time'].dt.hour/24\ndf['t2m'] +=273\ndf['longitude'] += 100\ndf[\"latitude\"] += 100\n# Chọn các cột đặc trưng và nhãn\nfeatures = df[['t2m']].values\n# features = df['t2m'].values.reshape(-1, 1)\n# labels = df['Fault_Label'].values  # Hoặc 'Fault_Type'\nlabels = df['Fault_Type'].values  # Hoặc 'Fault_Label'\n# print(features[:30])\n# print(df['t2m'])\n# # Chuẩn hóa dữ liệu\nscaler = StandardScaler()\n# scaler=MinMaxScaler(feature_range=(0, 1))\nfeatures = scaler.fit_transform(features)\n# # print(features[:30])\nsequence_length = 48\n# sequences = []\n# labels = []\nfeatures = features.reshape(-1)\n# # Iterate through the DataFrame in steps of `sequence_length` to avoid overlap\n# for i in range(0, len(df), sequence_length):\n#     # Check if there are enough data points left for a full sequence\n#     if i + sequence_length <= len(df):\n#         # Create a sequence of 5 consecutive data points as a 2D array (5x4)\n#         sequence = df.iloc[i:i+sequence_length][[\"t2m\"]].values\n#         sequences.append(sequence)\n        \n#         # Determine the label based on Fault_Label in the 5-point sequence\n#         label = 1 if df.iloc[i:i+sequence_length][\"Fault_Label\"].max() == 1 else 0\n#         labels.append(label)\n\n# # Convert to NumPy arrays for easier handling\n# X_seq = np.array(sequences)\n# y_seq = np.array(labels)\n\n\n# features = features.reshape(-1)\nnum_samples = len(features) - sequence_length + 1\n\n#Non - overlap\nX_seq = np.array([features[i*sequence_length:(i+1)*sequence_length] for i in range(num_samples//sequence_length)])\ny_seq = np.array([max(labels[i*sequence_length:(i+1)*sequence_length]) for i in range(num_samples//sequence_length)])\n\n#Overlap\n# X_seq = np.array([features[i:i+sequence_length] for i in range(num_samples)])\n# y_seq = np.array([max(labels[i:i+sequence_length]) for i in range(num_samples)])\n# y_seq = np.expand_dims(y_seq,axis=1)\n\n# # Splitting the data\nsplit_ratio = 0.1\nsplit_index = int(num_samples//sequence_length * split_ratio)\n# X_train, X_test = X_seq[split_index:], X_seq[:split_index]\n# y_train, y_test = y_seq[split_index:], y_seq[:split_index]\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.1, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\n# Converting to Tensors\nX_train_tensor = torch.FloatTensor(X_train)\ny_train_tensor = torch.LongTensor(y_train)\nX_test_tensor = torch.FloatTensor(X_test)\ny_test_tensor = torch.LongTensor(y_test)\nX_val_tensor = torch.FloatTensor(X_val)\ny_val_tensor = torch.LongTensor(y_val)\n\n\n# Creating TensorDataset and DataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Verify batch shape\nfor X_batch, y_batch in train_loader:\n    print(\"Feature batch:\", X_batch.shape)  # Should output [64, 5, 4]\n    print(\"Label batch:\", y_batch.shape)    # Should output [64]\n    # print(X_batch)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-12-12T11:02:22.483999Z","iopub.execute_input":"2024-12-12T11:02:22.484531Z","iopub.status.idle":"2024-12-12T11:02:28.763745Z","shell.execute_reply.started":"2024-12-12T11:02:22.484478Z","shell.execute_reply":"2024-12-12T11:02:28.762608Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Feature batch: torch.Size([64, 48])\nLabel batch: torch.Size([64])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"len(train_loader), len(val_loader), len(test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T07:47:25.862855Z","iopub.execute_input":"2024-12-06T07:47:25.863193Z","iopub.status.idle":"2024-12-06T07:47:25.869725Z","shell.execute_reply.started":"2024-12-06T07:47:25.863164Z","shell.execute_reply":"2024-12-06T07:47:25.868808Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(43, 3, 3)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"len(y_train_tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T09:16:52.573484Z","iopub.execute_input":"2024-11-29T09:16:52.574007Z","iopub.status.idle":"2024-11-29T09:16:52.581626Z","shell.execute_reply.started":"2024-11-29T09:16:52.573940Z","shell.execute_reply":"2024-11-29T09:16:52.580478Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"8211"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"X_seq.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:01:13.105623Z","iopub.execute_input":"2024-12-06T08:01:13.106426Z","iopub.status.idle":"2024-12-06T08:01:13.112132Z","shell.execute_reply.started":"2024-12-06T08:01:13.106390Z","shell.execute_reply":"2024-12-06T08:01:13.110983Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(3040, 72)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"len(y_test_tensor[y_test_tensor == 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:04:16.506457Z","iopub.execute_input":"2024-11-22T10:04:16.506793Z","iopub.status.idle":"2024-11-22T10:04:16.513111Z","shell.execute_reply.started":"2024-11-22T10:04:16.506766Z","shell.execute_reply":"2024-11-22T10:04:16.512262Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"2904"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"for X_batch, y_batch in test_loader:\n    print(\"Feature batch:\", X_batch.shape)  # Should output [64, 5, 4]\n    print(\"Label batch:\", y_batch.shape)    # Should output [64]\n    print(y_batch)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T09:24:33.344309Z","iopub.execute_input":"2024-11-15T09:24:33.345069Z","iopub.status.idle":"2024-11-15T09:24:33.351754Z","shell.execute_reply.started":"2024-11-15T09:24:33.345029Z","shell.execute_reply":"2024-11-15T09:24:33.350717Z"}},"outputs":[{"name":"stdout","text":"Feature batch: torch.Size([1, 5])\nLabel batch: torch.Size([1])\ntensor([0])\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"idx = 0\nfor X_batch, y_batch in train_loader:\n    if idx == 1:\n        print(\"Feature batch:\", X_batch)\n        print(\"Label batch:\", y_batch)\n        print(y_batch.shape)\n        break\n    idx +=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T06:51:54.393846Z","iopub.execute_input":"2024-11-15T06:51:54.394549Z","iopub.status.idle":"2024-11-15T06:51:54.402218Z","shell.execute_reply.started":"2024-11-15T06:51:54.394506Z","shell.execute_reply":"2024-11-15T06:51:54.401259Z"}},"outputs":[{"name":"stdout","text":"Feature batch: tensor([[-0.1213,  0.3206,  0.8189,  1.2644,  1.4409,  1.6575,  1.6548,  1.5200,\n          1.3435,  1.0766,  0.6729,  0.2983,  0.1522,  0.0723,  0.0036, -0.0629,\n         -0.1206, -0.1968, -0.2810, -0.3790, -0.4572, -0.5115, -0.5565, -0.5853]])\nLabel batch: tensor([0])\ntorch.Size([1])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from tqdm import tqdm\n\n# # Định nghĩa lớp RBM\n# class RBM(nn.Module):\n#     def __init__(self, visible_units, hidden_units, learning_rate=0.00005, device='cpu'):\n#         super(RBM, self).__init__()\n#         self.visible_units = visible_units\n#         self.hidden_units = hidden_units\n#         self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.001)\n#         self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n#         self.v_bias = nn.Parameter(torch.zeros(visible_units))\n#         self.learning_rate = learning_rate\n#         self.device = device\n#         self.to(device)  # Send model parameters to the specified device\n\n#     def forward(self, v):\n#         v = v.to(self.device)  # Ensure input is on the correct device\n#         h_prob = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n#         # print(\"h_prob\",h_prob)\n#         h_sample = torch.bernoulli(h_prob)\n#         return h_sample\n\n#     def backward(self, h):\n#         h = h.to(self.device)  # Ensure input is on the correct device\n#         v_prob = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n#         # print(\"v_prob\",v_prob)\n#         v_sample = torch.bernoulli(v_prob)\n#         return v_sample\n\n#     def train_rbm(self, dataloader, epochs=100):\n#         for epoch in tqdm(range(epochs), desc=\"Training RBM\"):\n#             idx = 0\n#             for X_batch,y_batch in dataloader:\n#                 v = X_batch.to(self.device)\n#                 h = self.forward(v)\n#                 v_reconstructed = self.backward(h)\n#                 # Tính gradient và cập nhật trọng số\n#                 positive_grad = torch.matmul(h.t(), v)\n#                 negative_grad = torch.matmul(h.t(), v_reconstructed)\n#                 # print(positive_grad, negative_grad)\n#                 # print(\"Before\",self.W.data)\n#                 # print(self.learning_rate * torch.sum(v - v_reconstructed, dim=0))\n#                 self.W.data += self.learning_rate * (positive_grad - negative_grad)\n#                 self.v_bias.data += self.learning_rate * torch.sum(v - v_reconstructed, dim=0)\n#                 self.h_bias.data += self.learning_rate * torch.sum(h - h, dim=0)\n#                 # print(\"After\",self.W.data)\n#                 # if(idx == 1 or idx == 5):\n#                 #     print(h)\n#                 # idx +=1\n                \n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:27:38.138371Z","iopub.execute_input":"2024-11-14T15:27:38.138688Z","iopub.status.idle":"2024-11-14T15:27:38.143176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Định nghĩa mô hình DBN\n# class DBN(nn.Module):\n#     def __init__(self, input_size, k, l,device ='cpu'):\n#         super(DBN, self).__init__()\n#         self.device = device\n#         self.rbm1 = RBM(input_size, 2 * k - 1,device =device)\n#         self.rbm2 = RBM(2 * k - 1, 2 * k - 1,device =device)\n#         self.bp_model = nn.Sequential(\n#             nn.Linear(2 * k - 1, l),\n#             nn.Softmax(dim=1)\n#         )\n\n#     def forward(self, x):\n#         x = self.rbm1.forward(x)\n#         x = self.rbm2.forward(x)\n#         x = self.bp_model(x)\n#         return x","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:27:38.144032Z","iopub.execute_input":"2024-11-14T15:27:38.144301Z","iopub.status.idle":"2024-11-14T15:27:38.153492Z","shell.execute_reply.started":"2024-11-14T15:27:38.144271Z","shell.execute_reply":"2024-11-14T15:27:38.152629Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# # Cấu hình mô hình\n# k = X_train.shape[1]  # Số đặc trưng trong dữ liệu\n# l = 2  # Số lượng lớp Softmax neurons\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)\n# dbn = DBN(input_size=k, k=k, l=l,device=device).to(device)\n\n# # Hàm mất mát và bộ tối ưu\n# criterion = nn.CrossEntropyLoss().to(device)\n# optimizer = optim.SGD(dbn.parameters(), lr=0.0005)\n\n# # Huấn luyện mô hình\n# epochs_rbm = 50  # Số epoch cho RBM\n# epochs_bp = 50   # Số epoch cho Backpropagation\n# batch_size = 32\n\n# # Huấn luyện RBM1\n# dbn.rbm1.train_rbm(train_loader, epochs=epochs_rbm)\n\n# # Lấy đầu ra của RBM1 để huấn luyện RBM2\n# hidden_outputs = []\n\n# with torch.no_grad():\n#     idx = 0\n#     for X_batch,y_batch in train_loader:\n#         # print(\"down\")\n#         v = X_batch.to(dbn.rbm1.device)  # Extract data and move to the device\n#         h = dbn.rbm1.forward(v)\n#         if idx == len(train_loader)-1:\n#              print(\"h\",h)\n#         idx +=1\n        \n#         hidden_outputs.append(h)\n        \n\n# # Concatenate all hidden outputs into a single tensor for RBM2 training\n# hidden_outputs = torch.cat(hidden_outputs, dim=0)\n\n# # Create a DataLoader for RBM2 training\n# hidden_dataset = TensorDataset(hidden_outputs,torch.zeros(hidden_outputs.shape[0]))\n# hidden_loader = DataLoader(hidden_dataset, batch_size=64, shuffle=False)\n# # Huấn luyện RBM2\n# dbn.rbm2.train_rbm(hidden_loader, epochs=epochs_rbm)\n\n# # Huấn luyện mô hình DBN\n# for epoch in range(epochs_bp):\n#     for X_batch, y_batch in train_loader:\n#         # Forward pass\n#         X_batch = X_batch.to(device)\n#         y_batch = y_batch.to(device)\n#         outputs = dbn(X_batch)\n#         # print(outputs.shape)\n#         # print(y_batch.shape)\n#         loss = criterion(outputs, y_batch.squeeze())\n\n#         # Backward pass và cập nhật trọng số\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         # break\n    \n#     print(f'Epoch {epoch+1}/{epochs_bp}, Loss: {loss.item()}')\n#     # break","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:27:38.154890Z","iopub.execute_input":"2024-11-14T15:27:38.155159Z","iopub.status.idle":"2024-11-14T15:27:38.231951Z","shell.execute_reply.started":"2024-11-14T15:27:38.155129Z","shell.execute_reply":"2024-11-14T15:27:38.229967Z"},"trusted":true,"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m----> 6\u001b[0m dbn \u001b[38;5;241m=\u001b[39m \u001b[43mDBN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Hàm mất mát và bộ tối ưu\u001b[39;00m\n\u001b[1;32m      9\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[47], line 6\u001b[0m, in \u001b[0;36mDBN.__init__\u001b[0;34m(self, input_size, k, l, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28msuper\u001b[39m(DBN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbm1 \u001b[38;5;241m=\u001b[39m \u001b[43mRBM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbm2 \u001b[38;5;241m=\u001b[39m RBM(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,device \u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbp_model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, l),\n\u001b[1;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m )\n","\u001b[0;31mTypeError\u001b[0m: RBM.__init__() got an unexpected keyword argument 'device'"],"ename":"TypeError","evalue":"RBM.__init__() got an unexpected keyword argument 'device'","output_type":"error"}],"execution_count":48},{"cell_type":"code","source":"# from sklearn.metrics import precision_score, recall_score, f1_score\n\n# # Ensure the model is in evaluation mode\n# dbn.eval()\n\n# # Initialize lists to collect predictions and labels for overall metrics\n# all_y_pred_classes = []\n# all_y_test_classes = []\n\n# with torch.no_grad():\n#     for X_batch, y_batch in test_loader:\n#         # Move inputs and labels to the appropriate device\n#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n#         # Get predictions\n#         y_pred = dbn(X_batch)\n#         y_pred_classes = torch.argmax(y_pred, dim=1)\n        \n#         # Calculate accuracy for this batch\n#         accuracy = torch.sum(y_pred_classes == y_batch).item() / len(y_batch)\n#         # print(f'Batch Accuracy: {accuracy * 100:.2f}%')\n        \n#         # Collect predictions and true labels for overall metrics\n#         all_y_pred_classes.extend(y_pred_classes.cpu().numpy())\n#         all_y_test_classes.extend(y_batch.cpu().numpy())\n\n# # Calculate overall precision, recall, and F1-score\n# precision = precision_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n# recall = recall_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n# f1 = f1_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n\n# print(f'Overall Precision: {precision:.2f}')\n# print(f'Overall Recall: {recall:.2f}')\n# print(f'Overall F1 Score: {f1:.2f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T15:27:38.233204Z","iopub.status.idle":"2024-11-14T15:27:38.233718Z","shell.execute_reply.started":"2024-11-14T15:27:38.233442Z","shell.execute_reply":"2024-11-14T15:27:38.233467Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### New RBm trainning","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import torch.nn.functional as F\n\n# class RBM(nn.Module):\n#     def __init__(self, n_visible, n_hidden):\n#         super(RBM, self).__init__()\n#         self.n_visible = n_visible\n#         self.n_hidden = n_hidden\n        \n#         # Weight and bias initialization\n#         self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)\n#         self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n#         self.v_bias = nn.Parameter(torch.zeros(n_visible))\n    \n#     def sample_from_p(self, p):\n#         \"\"\"Sample a binary state based on probabilities p.\"\"\"\n#         return F.relu(p - torch.rand(p.size(), device=p.device))\n    \n#     def forward(self, v):\n#         \"\"\"Perform one step of Gibbs sampling: visible to hidden to visible.\"\"\"\n#         h_prob = torch.relu(torch.mm(v, self.W.t())+ self.h_bias)\n#         # print(h_prob)\n#         h_sample = self.sample_from_p(h_prob)\n        \n#         v_prob = torch.relu(torch.mm(h_sample, self.W)+ self.v_bias)\n#         v_sample = self.sample_from_p(v_prob)\n#         return v_sample, h_prob, h_sample\n    \n#     def free_energy(self, v):\n#         \"\"\"Calculate the free energy for a given visible layer v.\"\"\"\n#         vbias_term = v.mv(self.v_bias)\n#         wx_b = F.relu(torch.mm(v, self.W.t()) + self.h_bias)\n#         hidden_term = torch.sum(F.softplus(wx_b), dim=1)\n#         return -vbias_term - hidden_term\n\n# def train_rbm(rbm, data_loader, n_epochs=10, learning_rate=0.001, k=1,device ='cpu'):\n#     \"\"\"Train an RBM using contrastive divergence.\"\"\"\n#     optimizer = optim.SGD(rbm.parameters(), lr=learning_rate)\n#     for epoch in range(n_epochs):\n#         epoch_loss = 0\n#         idx =0\n    \n#         for batch in data_loader:\n#             v, _ = batch\n#             v = v.view(-1, rbm.n_visible) # Flatten the input\n\n#             v = v.to(device)\n#             # Positive phase\n            \n#             v_sample, h_prob, h_sample = rbm(v)\n            \n#             for _ in range(k):\n#                 v_sample, h_prob, h_sample = rbm(v_sample)\n            \n#             # print(v_sample)\n            \n#             # break\n#             # Update weights and biases\n#             positive_grad = torch.mm(h_prob.t(), v)  # Hoán đổi thứ tự ma trận\n#             negative_grad = torch.mm(h_sample.t(),v_sample)  # Hoán đổi thứ tự ma trận\n#             # print(negative_grad)\n#             # break\n#             # if idx == len(data_loader) -1:\n#             #     print(v)\n#             #     print(v_sample)\n#             #     print(h)\n#             #     print(h_sample)\n#             optimizer.zero_grad()\n#             rbm.W.grad = (positive_grad - negative_grad) / v.size(0)\n#             rbm.v_bias.grad =  torch.sum(v - v_sample, dim=0) / v.size(0)\n#             rbm.h_bias.grad = torch.sum(h_prob - h_sample, dim=0) / v.size(0)\n#             optimizer.step()\n#             epoch_loss += torch.mean(rbm.free_energy(v) - rbm.free_energy(v_sample))\n#             idx +=1\n#         print(rbm.W)\n    \n#         print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss.item()}\")\n\n# # Example usage:\n# # Initialize the RBM and data loader\n# n_visible = 4  # Example for MNIST (28*28)\n# n_hidden = 2 * n_visible -1   # Number of hidden units\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# rbm1 = RBM(n_visible, n_hidden)\n# rbm1.to(device)\n# rbm2 = RBM(n_hidden, n_hidden)\n\n# # Assuming data_loader is defined and provides batches of MNIST data\n# train_rbm(rbm1, train_loader, n_epochs=20, learning_rate=0.005, k=1,device=device)\n\n# hidden_outputs = []\n# with torch.no_grad():\n#     for batch in train_loader:\n#         v, _ = batch\n#         v = v.view(-1, rbm1.n_visible) # Flatten the input\n#         v = v.to(device)\n            \n#         v_sample, h_prob, h_sample = rbm1(v)\n        \n#         hidden_outputs.append(h_prob)\n# hidden_outputs = torch.cat(hidden_outputs, dim=0)\n\n# # Create a DataLoader for RBM2 training\n# hidden_dataset = TensorDataset(hidden_outputs,torch.zeros(hidden_outputs.shape[0]))\n# hidden_loader = DataLoader(hidden_dataset, batch_size=64, shuffle=False)\n# rbm2.to(device)\n# train_rbm(rbm2, hidden_loader, n_epochs=20, learning_rate=0.005, k=1,device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:27:38.235001Z","iopub.status.idle":"2024-11-14T15:27:38.235351Z","shell.execute_reply.started":"2024-11-14T15:27:38.235177Z","shell.execute_reply":"2024-11-14T15:27:38.235195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RBM with 4 feauture","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import numpy as np\n# import torch.nn.functional as F\n# from tqdm import trange\n\n# class RBM(nn.Module):\n#     def __init__(self, n_visible, n_hidden, lr=0.001, epochs=5, mode='bernoulli', batch_size=32, k=3, optimizer='adam', gpu=False, savefile=None, early_stopping_patience=5):\n#         super(RBM, self).__init__()\n#         self.mode = mode  # 'bernoulli' or 'gaussian' RBM\n#         self.n_hidden = n_hidden\n#         self.n_visible = n_visible\n#         self.lr = lr\n#         self.epochs = epochs\n#         self.batch_size = batch_size\n#         self.k = k\n#         self.optimizer = optimizer\n#         self.beta_1 = 0.9\n#         self.beta_2 = 0.999\n#         self.epsilon = 1e-7\n#         self.m = [0, 0, 0]\n#         self.v = [0, 0, 0]\n#         self.savefile = savefile\n#         self.early_stopping_patience = early_stopping_patience\n#         self.stagnation = 0\n#         self.previous_loss = float('inf')\n#         self.progress = []\n\n#         # Set device to GPU if available and specified\n#         self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and gpu else \"cpu\")\n\n#         # Initialize weights and biases\n#         std = 4 * np.sqrt(6. / (self.n_visible + self.n_hidden))\n#         self.W = torch.normal(mean=0, std=std, size=(self.n_hidden, self.n_visible), device=self.device)\n#         self.vb = torch.zeros(size=(1,self.n_visible), device=self.device)\n#         self.hb = torch.zeros(size=(1,self.n_hidden), device=self.device)\n\n#     def sample_h(self, x):\n#         wx = torch.bmm(self.W.unsqueeze(0).repeat(x.size(0), 1, 1),x)\n#         activation = wx + self.hb.unsqueeze(-1)\n#         p_h_given_v = torch.sigmoid(activation)\n#         if self.mode == 'bernoulli':\n#             return p_h_given_v, torch.bernoulli(p_h_given_v)\n#         else:\n#             random_noise = torch.normal(mean=0, std=1, size=p_h_given_v.shape, device=self.device)\n#             return_value = p_h_given_v +random_nose\n#             del random_noise\n#             return p_h_given_v, return_value\n\n#     def sample_v(self, y):\n        \n#         wy = torch.bmm(self.W.t().unsqueeze(0).repeat(y.size(0), 1, 1),y)\n#         activation = wy + self.vb.unsqueeze(-1)\n#         p_v_given_h = torch.sigmoid(activation)\n#         del wy,activation\n#         if self.mode == 'bernoulli':\n#             return p_v_given_h, torch.bernoulli(p_v_given_h)\n#         else:\n#             return p_v_given_h, p_v_given_h + torch.normal(mean=0, std=1, size=p_v_given_h.shape, device=self.device)\n\n#     def adam(self, g, epoch, index):\n#         self.m[index] = self.beta_1 * self.m[index] + (1 - self.beta_1) * g\n#         self.v[index] = self.beta_2 * self.v[index] + (1 - self.beta_2) * torch.pow(g, 2)\n\n#         m_hat = self.m[index] / (1 - np.power(self.beta_1, epoch))\n#         v_hat = self.v[index] / (1 - np.power(self.beta_2, epoch))\n#         return m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n#     def update(self, v0, vk, ph0, phk, epoch):\n#         dW = (torch.bmm(ph0,v0.transpose(1, 2)) - torch.bmm(phk,vk.transpose(1, 2))).sum(dim=0)\n#         dvb = torch.sum(v0 - vk, dim=0).sum(dim=-1)\n#         dhb = torch.sum(ph0 - phk, dim=0).sum(dim=-1)\n\n#         if self.optimizer == 'adam':\n#             dW = self.adam(dW, epoch, 0)\n#             dvb = self.adam(dvb, epoch, 1)\n#             dhb = self.adam(dhb, epoch, 2)\n            \n#         with torch.no_grad():\n#             self.W.add_(self.lr * dW) \n#             self.vb.add_(self.lr * dvb)  \n#             self.hb.add_(self.lr * dhb)\n        \n\n#     # def train(self, dataset):\n#     #     # Start training progress bar\n#     #     learning = trange(self.epochs, desc=\"Starting...\")\n        \n#     #     for epoch in learning:\n#     #         train_loss = 0\n#     #         counter = 0\n            \n#     #         # Loop through dataset in batches\n#     #         for batch_start_index in range(0, dataset.shape[0] - self.batch_size, self.batch_size):\n#     #             # Extract batch and transfer to device\n#     #             v0 = dataset[batch_start_index:batch_start_index + self.batch_size].to(self.device)\n#     #             vk = v0.clone()  # Initialize vk with the same values as v0\n                \n#     #             # Positive phase\n#     #             ph0, _ = self.sample_h(v0)\n                \n#     #             # Gibbs sampling (k-steps)\n#     #             for _ in range(self.k):\n#     #                 _, hk = self.sample_h(vk)\n#     #                 _, vk = self.sample_v(hk)\n                \n#     #             # Negative phase\n#     #             phk, _ = self.sample_h(vk)\n                \n#     #             # Update weights and biases\n#     #             self.update(v0, vk, ph0, phk, epoch + 1)\n                \n#     #             # Accumulate training loss\n#     #             train_loss += torch.mean(torch.abs(v0 - vk)).item()\n#     #             counter += 1\n            \n#     #         # Average loss for the epoch\n#     #         avg_loss = train_loss / counter\n#     #         self.progress.append(avg_loss)\n#     #         learning.set_description(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}\")\n#     #         learning.refresh()\n            \n#     #         # Early stopping logic\n#     #         if avg_loss > self.previous_loss and epoch > self.early_stopping_patience:\n#     #             self.stagnation += 1\n#     #             if self.stagnation >= self.early_stopping_patience:\n#     #                 print(\"Early stopping: Loss not improving.\")\n#     #                 break\n#     #         else:\n#     #             self.previous_loss = avg_loss\n#     #             self.stagnation = 0\n        \n#     #     learning.close()\n\n#     def train_new(self, data_loader):\n#         learning = trange(self.epochs, desc=\"Starting...\")\n#         for epoch in learning:\n#             train_loss = 0\n#             counter = 0\n#             for batch in data_loader:\n#                 v, _ = batch\n#                 v = v.to(self.device)\n#                 # Positive phase\n                \n#                 ph0, _ = self.sample_h(v)\n            \n#                 # Gibbs sampling (k-steps)\n#                 vk = v\n#                 for _ in range(self.k):\n#                     _, hk = self.sample_h(vk)\n#                     _, vk = self.sample_v(hk)\n\n#                 # Negative phase\n#                 phk, _ = self.sample_h(vk)\n                \n#                 # Update weights and biases\n                \n#                 self.update(v, vk, ph0, phk, epoch + 1)\n                \n#                 train_loss += torch.mean(torch.abs(v - vk)).item()\n#                 counter += 1\n#                 del v, vk, ph0, phk, hk\n#                 torch.cuda.empty_cache()\n#                 # del tensor  # Delete the tensor\n                \n#             avg_loss = train_loss / counter\n#             self.progress.append(avg_loss)\n#             learning.set_description(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}\")\n#             learning.refresh()\n\n#             # Early stopping check\n#             if avg_loss > self.previous_loss and epoch > self.early_stopping_patience:\n#                 self.stagnation += 1\n#                 if self.stagnation >= self.early_stopping_patience:\n#                     print(\"Early stopping: Loss is not improving.\")\n#                     break\n#             else:\n#                 self.previous_loss = avg_loss\n#                 self.stagnation = 0\n#         learning.close()\n\n# # Parameters\n# n_visible = 5  # Number of visible units\n# n_hidden = 2 * n_visible - 1  # Number of hidden units\n# epochs = 50\n# lr = 0.005\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# # Initialize the RBM\n# rbm1 = RBM(n_visible, n_hidden, epochs=epochs, mode='bernoulli', lr=lr, optimizer='adam', gpu=True, early_stopping_patience=5)\n\n# # Assume `train_loader` is defined as a DataLoader for the training dataset\n# rbm1.train_new(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T08:36:31.487955Z","iopub.execute_input":"2024-11-15T08:36:31.488663Z","iopub.status.idle":"2024-11-15T08:36:31.660154Z","shell.execute_reply.started":"2024-11-15T08:36:31.488622Z","shell.execute_reply":"2024-11-15T08:36:31.658943Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Starting...:   0%|          | 0/50 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 193\u001b[0m\n\u001b[1;32m    190\u001b[0m rbm1 \u001b[38;5;241m=\u001b[39m RBM(n_visible, n_hidden, epochs\u001b[38;5;241m=\u001b[39mepochs, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbernoulli\u001b[39m\u001b[38;5;124m'\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Assume `train_loader` is defined as a DataLoader for the training dataset\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[43mrbm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 146\u001b[0m, in \u001b[0;36mRBM.train_new\u001b[0;34m(self, data_loader)\u001b[0m\n\u001b[1;32m    143\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Positive phase\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m ph0, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Gibbs sampling (k-steps)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m vk \u001b[38;5;241m=\u001b[39m v\n","Cell \u001b[0;32mIn[50], line 40\u001b[0m, in \u001b[0;36mRBM.sample_h\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_h\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 40\u001b[0m     wx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     activation \u001b[38;5;241m=\u001b[39m wx \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhb\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     p_h_given_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(activation)\n","\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"],"ename":"RuntimeError","evalue":"batch2 must be a 3D tensor","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"# rbm2 = RBM(n_hidden, n_hidden, epochs=epochs, mode='bernoulli', lr=lr, optimizer='adam', gpu=True, early_stopping_patience=5)\n\n# hidden_outputs = []\n\n# with torch.no_grad():\n#     idx = 0\n#     for X_batch,y_batch in train_loader:\n#         # print(\"down\")\n#         v = X_batch.to(device)  # Extract data and move to the device\n#         _, h= rbm1.sample_h(v)\n#         hidden_outputs.append(h)\n        \n\n# # Concatenate all hidden outputs into a single tensor for RBM2 training\n# hidden_outputs = torch.cat(hidden_outputs, dim=0)\n# hidden_dataset = TensorDataset(hidden_outputs,torch.zeros(hidden_outputs.shape[0]))\n# hidden_loader = DataLoader(hidden_dataset, batch_size=64, shuffle=False)\n\n# rbm2.train_new(hidden_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:27:38.239815Z","iopub.status.idle":"2024-11-14T15:27:38.240170Z","shell.execute_reply.started":"2024-11-14T15:27:38.239997Z","shell.execute_reply":"2024-11-14T15:27:38.240015Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class DBN(nn.Module):\n#     def __init__(self,k, rbm = [rbm1,rbm2],device ='cpu'):\n#         super(DBN, self).__init__()\n#         self.device = device\n#         self.k = k\n#         self.rbm1 = rbm1.to(self.device)\n#         self.rbm2 = rbm2.to(self.device)\n#         self.flatten = nn.Flatten()\n#         self.bp_model = nn.Sequential(\n#             nn.Linear((2 * self.k - 1), 5),\n#             nn.Softmax(dim=1)\n#         )\n#         self.rbm1.W = nn.Parameter(self.rbm1.W)\n#         self.rbm1.vb = nn.Parameter(self.rbm1.vb)\n#         self.rbm1.hb = nn.Parameter(self.rbm1.hb)\n\n#         self.rbm2.W = nn.Parameter(self.rbm2.W)\n#         self.rbm2.vb = nn.Parameter(self.rbm2.vb)\n#         self.rbm2.hb = nn.Parameter(self.rbm2.hb)\n\n#     def forward(self, x):\n#         _,x = self.rbm1.sample_h(x)\n#         _,x = self.rbm2.sample_h(x)\n#         x = self.flatten(x)\n#         x = self.bp_model(x)\n#         return x\n        \n# class FocalLoss(nn.Module):\n#     def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n#         \"\"\"\n#         :param alpha: Weighting factor for the class imbalance (float).\n#         :param gamma: Focusing parameter to reduce the impact of easy examples (float).\n#         :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n#         \"\"\"\n#         super(FocalLoss, self).__init__()\n#         self.alpha = alpha\n#         self.gamma = gamma\n#         self.reduction = reduction\n\n#     def forward(self, inputs, targets):\n#         \"\"\"\n#         :param inputs: Predicted probabilities (logits), shape [batch_size, num_classes].\n#         :param targets: Ground truth labels, shape [batch_size].\n#         \"\"\"\n#         # Apply softmax to inputs\n#         probas = F.softmax(inputs, dim=1)\n        \n#         # Gather probabilities corresponding to the true class\n#         targets_one_hot = F.one_hot(targets, num_classes=probas.size(1)).float()\n#         probas = probas.clamp(min=1e-8, max=1.0)  # Avoid log(0) issues\n#         pt = (probas * targets_one_hot).sum(dim=1)\n        \n#         # Compute the Focal Loss\n#         focal_loss = -self.alpha * ((1 - pt) ** self.gamma) * torch.log(pt)\n        \n#         # Apply reduction method\n#         if self.reduction == 'mean':\n#             return focal_loss.mean()\n#         elif self.reduction == 'sum':\n#             return focal_loss.sum()\n#         else:  # 'none'\n#             return focal_loss\n\n\n# dbn = DBN(k = 24, rbm =[rbm1,rbm2], device = device).to(device)\n\n# epochs_bp = 20\n# criterion = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean').to(device)\n# # criterion = nn.CrossEntropyLoss().to(device)\n# optimizer = optim.SGD(dbn.parameters(), lr=0.0005)\n# dbn.train()\n# for epoch in range(epochs_bp):\n#     for X_batch, y_batch in train_loader:\n#         # Forward pass\n#         X_batch = X_batch.to(device)\n#         y_batch = y_batch.to(device)\n#         outputs = dbn(X_batch)\n#         # print(outputs.shape)\n#         # print(y_batch.shape)\n#         loss = criterion(outputs, y_batch.squeeze())\n\n#         # Backward pass và cập nhật trọng số\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         # break\n    \n#     print(f'Epoch {epoch+1}/{epochs_bp}, Loss: {loss.item()}')\n#     # break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:27:38.241822Z","iopub.status.idle":"2024-11-14T15:27:38.242177Z","shell.execute_reply.started":"2024-11-14T15:27:38.242004Z","shell.execute_reply":"2024-11-14T15:27:38.242021Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import precision_score, recall_score, f1_score\n\n# # Ensure the model is in evaluation mode\n\n# # Initialize lists to collect predictions and labels for overall metrics\n# all_y_pred_classes = []\n# all_y_test_classes = []\n# dbn.eval()\n# with torch.no_grad():\n#     idx = 0\n#     for X_batch, y_batch in test_loader:\n#         # Move inputs and labels to the appropriate device\n#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n#         # Get predictions\n#         y_pred = dbn(X_batch)\n#         y_pred_classes = torch.argmax(y_pred, dim=1)\n#         print(y_pred_classes,y_batch)\n#         idx +=1\n#         if(idx == 4):\n#             break\n#         # Calculate accuracy for this batch\n#         accuracy = torch.sum(y_pred_classes == y_batch).item() / len(y_batch)\n#         # print(f'Batch Accuracy: {accuracy * 100:.2f}%')\n        \n#         # Collect predictions and true labels for overall metrics\n#         all_y_pred_classes.extend(y_pred_classes.cpu().numpy())\n#         all_y_test_classes.extend(y_batch.cpu().numpy())\n\n# # Calculate overall precision, recall, and F1-score\n# precision = precision_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n# recall = recall_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n# f1 = f1_score(all_y_test_classes, all_y_pred_classes, average='weighted')\n\n# print(f'Overall Precision: {precision:.2f}')\n# print(f'Overall Recall: {recall:.2f}')\n# print(f'Overall F1 Score: {f1:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:27:38.243563Z","iopub.status.idle":"2024-11-14T15:27:38.243943Z","shell.execute_reply.started":"2024-11-14T15:27:38.243772Z","shell.execute_reply":"2024-11-14T15:27:38.243789Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RBM with 1 feature","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm import trange\n\nclass RBM(nn.Module):\n    def __init__(self, n_visible, n_hidden, lr=0.001, epochs=5, mode='bernoulli', batch_size=32, k=3, optimizer='adam', gpu=False, savefile=None, early_stopping_patience=5):\n        super(RBM, self).__init__()\n        self.mode = mode  # 'bernoulli' or 'gaussian' RBM\n        self.n_hidden = n_hidden\n        self.n_visible = n_visible\n        self.lr = lr\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.k = k\n        self.optimizer = optimizer\n        self.beta_1 = 0.9\n        self.beta_2 = 0.999\n        self.epsilon = 1e-7\n        self.m = [0, 0, 0]\n        self.v = [0, 0, 0]\n        self.savefile = savefile\n        self.early_stopping_patience = early_stopping_patience\n        self.stagnation = 0\n        self.previous_loss = float('inf')\n        self.progress = []\n\n        # Set device to GPU if available and specified\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and gpu else \"cpu\")\n\n        # Initialize weights and biases\n        std = 4 * np.sqrt(6. / (self.n_visible + self.n_hidden))\n        self.W = torch.normal(mean=0, std=std, size=(self.n_hidden, self.n_visible), device=self.device)\n        self.vb = torch.zeros(size=(1,self.n_visible), device=self.device)\n        self.hb = torch.zeros(size=(1,self.n_hidden), device=self.device)\n\n    def sample_h(self, x):\n        wx = torch.mm(x, self.W.t())\n        activation = wx - self.hb\n        p_h_given_v = torch.relu(activation)\n        if self.mode == 'bernoulli':\n            return p_h_given_v, torch.bernoulli(p_h_given_v)\n        else:\n            # random_noise = torch.normal(mean=0, std=1, size=p_h_given_v.shape, device=self.device)\n            return_value = p_h_given_v\n            # del random_noise\n            return p_h_given_v, return_value\n\n    def sample_v(self, y):\n        \n        wy = torch.mm(y, self.W)\n        activation = wy - self.vb\n        p_v_given_h =torch.relu(activation)\n        if self.mode == 'bernoulli':\n            return p_v_given_h, torch.bernoulli(p_v_given_h)\n        else:\n            return p_v_given_h, p_v_given_h\n\n    def adam(self, g, epoch, index):\n        self.m[index] = self.beta_1 * self.m[index] + (1 - self.beta_1) * g\n        self.v[index] = self.beta_2 * self.v[index] + (1 - self.beta_2) * torch.pow(g, 2)\n\n        m_hat = self.m[index] / (1 - np.power(self.beta_1, epoch))\n        v_hat = self.v[index] / (1 - np.power(self.beta_2, epoch))\n        return m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n    def forward(self, x):\n        wx = torch.mm(x, self.W.t())\n        activation = wx - self.hb\n        p_h_given_v = torch.relu(activation)\n        return p_h_given_v\n\n    def update(self, v0, vk, ph0, phk, epoch):\n        dW = (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n        dvb = torch.sum((v0 - vk), 0)\n        dhb = torch.sum((ph0 - phk), 0)\n\n        if self.optimizer == 'adam':\n            dW = self.adam(dW, epoch, 0)\n            dvb = self.adam(dvb, epoch, 1)\n            dhb = self.adam(dhb, epoch, 2)\n            \n        with torch.no_grad():\n            self.W.add_(self.lr * dW) \n            self.vb.add_(self.lr * dvb)  \n            self.hb.add_(self.lr * dhb)\n\n    def train_new(self, data_loader):\n        learning = trange(self.epochs, desc=\"Starting...\")\n        for epoch in learning:\n            train_loss = 0\n            counter = 0\n            for batch in data_loader:\n                v, _ = batch\n                v = v.to(self.device)\n                # Positive phase\n\n                ph0, _ = self.sample_h(v)\n            \n                # Gibbs sampling (k-steps)\n                vk = v\n                for _ in range(self.k):\n                    _, hk = self.sample_h(vk)\n                    _, vk = self.sample_v(hk)\n\n                # Negative phase\n                phk, _ = self.sample_h(vk)\n                \n                # Update weights and biases\n                \n                self.update(v, vk, ph0, phk, epoch + 1)\n                \n                train_loss += torch.mean(torch.abs(v - vk)).item()\n                counter += 1\n                # del tensor  # Delete the tensor\n                \n            avg_loss = train_loss / counter\n            self.progress.append(avg_loss)\n            learning.set_description(f\"Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}\")\n            learning.refresh()\n\n            # Early stopping check\n            if avg_loss > self.previous_loss and epoch > self.early_stopping_patience:\n                self.stagnation += 1\n                if self.stagnation >= self.early_stopping_patience:\n                    print(\"Early stopping: Loss is not improving.\")\n                    break\n            else:\n                self.previous_loss = avg_loss\n                self.stagnation = 0\n        learning.close()\n\n# Parameters\nn_visible = X_seq.shape[1]  # Number of visible units\nn_hidden = 2 * n_visible - 1  # Number of hidden units\nepochs = 100\nlr = 0.005\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# Initialize the RBM\nrbm1 = RBM(n_visible, n_hidden, epochs=epochs, mode='none', lr=lr, optimizer='adam', gpu=True, early_stopping_patience=5)\n\n# Assume `train_loader` is defined as a DataLoader for the training dataset\nrbm1.train_new(train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:31:55.168751Z","iopub.execute_input":"2024-12-06T09:31:55.169108Z","iopub.status.idle":"2024-12-06T09:32:06.273739Z","shell.execute_reply.started":"2024-12-06T09:31:55.169074Z","shell.execute_reply":"2024-12-06T09:32:06.272773Z"}},"outputs":[{"name":"stderr","text":"Epoch 100/100, Loss: 291.5284: 100%|██████████| 100/100 [00:11<00:00,  9.04it/s]\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"rbm2 = RBM(n_hidden, n_hidden, epochs=epochs, mode='none', lr=lr, optimizer='adam', gpu=True, early_stopping_patience=5)\n\nhidden_outputs = []\n\nwith torch.no_grad():\n    idx = 0\n    for X_batch,y_batch in train_loader:\n        # print(\"down\")\n        v = X_batch.to(device)  # Extract data and move to the device\n        h= rbm1(v)\n        hidden_outputs.append(h)\n        \n\n# Concatenate all hidden outputs into a single tensor for RBM2 training\nhidden_outputs = torch.cat(hidden_outputs, dim=0)\nhidden_dataset = TensorDataset(hidden_outputs,torch.zeros(hidden_outputs.shape[0]))\nhidden_loader = DataLoader(hidden_dataset, batch_size=64, shuffle=False)\n\nrbm2.train_new(hidden_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:32:06.275537Z","iopub.execute_input":"2024-12-06T09:32:06.275901Z","iopub.status.idle":"2024-12-06T09:32:17.363846Z","shell.execute_reply.started":"2024-12-06T09:32:06.275862Z","shell.execute_reply":"2024-12-06T09:32:17.362945Z"}},"outputs":[{"name":"stderr","text":"Epoch 100/100, Loss: 8563.3220: 100%|██████████| 100/100 [00:11<00:00,  9.07it/s]\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"class DBN(nn.Module):\n    def __init__(self, k, rbm_layers=[], device='cpu'):\n        super(DBN, self).__init__()\n        self.device = device\n        self.k = k\n        self.rbm_layers = nn.ModuleList([rbm.to(self.device) for rbm in rbm_layers])  # Use ModuleList for RBM layers\n        self.flatten = nn.Flatten()\n        \n        # Define the backpropagation model\n        self.bp_model = nn.Sequential(\n            nn.Linear(2 * self.k - 1, 5),\n            nn.Softmax(dim=1)\n        )\n\n        # Register RBM parameters\n        for i, rbm in enumerate(self.rbm_layers):\n            self.register_parameter(f'rbm{i+1}_W', nn.Parameter(rbm.W))\n            self.register_parameter(f'rbm{i+1}_vb', nn.Parameter(rbm.vb))\n            self.register_parameter(f'rbm{i+1}_hb', nn.Parameter(rbm.hb))\n            \n    def forward(self, x):\n        # Pass input through each RBM layer sequentially\n        for rbm in self.rbm_layers:\n            x = rbm(x)\n        x = self.flatten(x)\n        x = self.bp_model(x)\n        return x\n        \nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n        \"\"\"\n        :param alpha: Weighting factor for the class imbalance (float).\n        :param gamma: Focusing parameter to reduce the impact of easy examples (float).\n        :param reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        :param inputs: Predicted probabilities (logits), shape [batch_size, num_classes].\n        :param targets: Ground truth labels, shape [batch_size].\n        \"\"\"\n        # Apply softmax to inputs\n        probas = F.softmax(inputs, dim=1)\n        \n        # Gather probabilities corresponding to the true class\n        targets_one_hot = F.one_hot(targets, num_classes=probas.size(1)).float()\n        probas = probas.clamp(min=1e-8, max=1.0)  # Avoid log(0) issues\n        pt = (probas * targets_one_hot).sum(dim=1)\n        \n        # Compute the Focal Loss\n        focal_loss = -self.alpha * ((1 - pt) ** self.gamma) * torch.log(pt)\n        \n        # Apply reduction method\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:  # 'none'\n            return focal_loss\n   \n\ndbn = DBN(k = X_seq.shape[1], rbm_layers =[rbm1,rbm2], device = device).to(device)\n\nepochs_bp = 500\n# criterion = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean').to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(dbn.parameters(), lr=0.01)\ndbn.train()\nfor epoch in range(epochs_bp):\n    for X_batch, y_batch in train_loader:\n        # Forward pass\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        outputs = dbn(X_batch)\n        # print(outputs.shape)\n        loss = criterion(outputs, y_batch)\n\n        # Backward pass và cập nhật trọng số\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # break\n    \n    print(f'Epoch {epoch+1}/{epochs_bp}, Loss: {loss.item()}')\n    # break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:28:01.749560Z","iopub.execute_input":"2024-12-06T08:28:01.749922Z","iopub.status.idle":"2024-12-06T08:28:34.285379Z","shell.execute_reply.started":"2024-12-06T08:28:01.749893Z","shell.execute_reply":"2024-12-06T08:28:34.284487Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1/500, Loss: 1.4894434213638306\nEpoch 2/500, Loss: 1.5154458284378052\nEpoch 3/500, Loss: 1.4693611860275269\nEpoch 4/500, Loss: 1.4543246030807495\nEpoch 5/500, Loss: 1.5129801034927368\nEpoch 6/500, Loss: 1.4535670280456543\nEpoch 7/500, Loss: 1.387885570526123\nEpoch 8/500, Loss: 1.4080990552902222\nEpoch 9/500, Loss: 1.4421690702438354\nEpoch 10/500, Loss: 1.4947739839553833\nEpoch 11/500, Loss: 1.3887572288513184\nEpoch 12/500, Loss: 1.3517156839370728\nEpoch 13/500, Loss: 1.5608621835708618\nEpoch 14/500, Loss: 1.5292372703552246\nEpoch 15/500, Loss: 1.4220224618911743\nEpoch 16/500, Loss: 1.3847893476486206\nEpoch 17/500, Loss: 1.4405876398086548\nEpoch 18/500, Loss: 1.3914653062820435\nEpoch 19/500, Loss: 1.468755841255188\nEpoch 20/500, Loss: 1.3673518896102905\nEpoch 21/500, Loss: 1.4024906158447266\nEpoch 22/500, Loss: 1.4058643579483032\nEpoch 23/500, Loss: 1.411859393119812\nEpoch 24/500, Loss: 1.4519376754760742\nEpoch 25/500, Loss: 1.3652238845825195\nEpoch 26/500, Loss: 1.4201537370681763\nEpoch 27/500, Loss: 1.3134199380874634\nEpoch 28/500, Loss: 1.4541187286376953\nEpoch 29/500, Loss: 1.490613341331482\nEpoch 30/500, Loss: 1.3766597509384155\nEpoch 31/500, Loss: 1.4600266218185425\nEpoch 32/500, Loss: 1.435750126838684\nEpoch 33/500, Loss: 1.5311757326126099\nEpoch 34/500, Loss: 1.447974681854248\nEpoch 35/500, Loss: 1.5296119451522827\nEpoch 36/500, Loss: 1.382344126701355\nEpoch 37/500, Loss: 1.3793487548828125\nEpoch 38/500, Loss: 1.3972395658493042\nEpoch 39/500, Loss: 1.3578780889511108\nEpoch 40/500, Loss: 1.3948429822921753\nEpoch 41/500, Loss: 1.403539776802063\nEpoch 42/500, Loss: 1.4316010475158691\nEpoch 43/500, Loss: 1.4029306173324585\nEpoch 44/500, Loss: 1.5510765314102173\nEpoch 45/500, Loss: 1.33653724193573\nEpoch 46/500, Loss: 1.2402281761169434\nEpoch 47/500, Loss: 1.386757493019104\nEpoch 48/500, Loss: 1.3352723121643066\nEpoch 49/500, Loss: 1.3739322423934937\nEpoch 50/500, Loss: 1.4809389114379883\nEpoch 51/500, Loss: 1.3185139894485474\nEpoch 52/500, Loss: 1.3898977041244507\nEpoch 53/500, Loss: 1.4423011541366577\nEpoch 54/500, Loss: 1.2767266035079956\nEpoch 55/500, Loss: 1.494423747062683\nEpoch 56/500, Loss: 1.4376188516616821\nEpoch 57/500, Loss: 1.3624953031539917\nEpoch 58/500, Loss: 1.3755356073379517\nEpoch 59/500, Loss: 1.413819432258606\nEpoch 60/500, Loss: 1.3694261312484741\nEpoch 61/500, Loss: 1.3512091636657715\nEpoch 62/500, Loss: 1.4030596017837524\nEpoch 63/500, Loss: 1.3536615371704102\nEpoch 64/500, Loss: 1.4558199644088745\nEpoch 65/500, Loss: 1.289313554763794\nEpoch 66/500, Loss: 1.2532304525375366\nEpoch 67/500, Loss: 1.4267921447753906\nEpoch 68/500, Loss: 1.4573979377746582\nEpoch 69/500, Loss: 1.2718244791030884\nEpoch 70/500, Loss: 1.2182697057724\nEpoch 71/500, Loss: 1.4330865144729614\nEpoch 72/500, Loss: 1.3063613176345825\nEpoch 73/500, Loss: 1.3090771436691284\nEpoch 74/500, Loss: 1.355796217918396\nEpoch 75/500, Loss: 1.3298105001449585\nEpoch 76/500, Loss: 1.4990288019180298\nEpoch 77/500, Loss: 1.2760043144226074\nEpoch 78/500, Loss: 1.4205288887023926\nEpoch 79/500, Loss: 1.4066647291183472\nEpoch 80/500, Loss: 1.4576090574264526\nEpoch 81/500, Loss: 1.4435230493545532\nEpoch 82/500, Loss: 1.3001261949539185\nEpoch 83/500, Loss: 1.4025682210922241\nEpoch 84/500, Loss: 1.42495596408844\nEpoch 85/500, Loss: 1.332596778869629\nEpoch 86/500, Loss: 1.4665812253952026\nEpoch 87/500, Loss: 1.331478476524353\nEpoch 88/500, Loss: 1.3633826971054077\nEpoch 89/500, Loss: 1.462714672088623\nEpoch 90/500, Loss: 1.3848594427108765\nEpoch 91/500, Loss: 1.310630440711975\nEpoch 92/500, Loss: 1.4043196439743042\nEpoch 93/500, Loss: 1.3733686208724976\nEpoch 94/500, Loss: 1.392773985862732\nEpoch 95/500, Loss: 1.3590551614761353\nEpoch 96/500, Loss: 1.3625437021255493\nEpoch 97/500, Loss: 1.244630217552185\nEpoch 98/500, Loss: 1.4182714223861694\nEpoch 99/500, Loss: 1.3400278091430664\nEpoch 100/500, Loss: 1.3575087785720825\nEpoch 101/500, Loss: 1.3237452507019043\nEpoch 102/500, Loss: 1.2669676542282104\nEpoch 103/500, Loss: 1.4094476699829102\nEpoch 104/500, Loss: 1.3139634132385254\nEpoch 105/500, Loss: 1.315049409866333\nEpoch 106/500, Loss: 1.2368332147598267\nEpoch 107/500, Loss: 1.3444105386734009\nEpoch 108/500, Loss: 1.3200256824493408\nEpoch 109/500, Loss: 1.4316095113754272\nEpoch 110/500, Loss: 1.4881020784378052\nEpoch 111/500, Loss: 1.3861550092697144\nEpoch 112/500, Loss: 1.3727144002914429\nEpoch 113/500, Loss: 1.4544540643692017\nEpoch 114/500, Loss: 1.3492523431777954\nEpoch 115/500, Loss: 1.3554166555404663\nEpoch 116/500, Loss: 1.3630322217941284\nEpoch 117/500, Loss: 1.3740696907043457\nEpoch 118/500, Loss: 1.3295955657958984\nEpoch 119/500, Loss: 1.3372005224227905\nEpoch 120/500, Loss: 1.211426854133606\nEpoch 121/500, Loss: 1.3698234558105469\nEpoch 122/500, Loss: 1.3298534154891968\nEpoch 123/500, Loss: 1.2017346620559692\nEpoch 124/500, Loss: 1.3182185888290405\nEpoch 125/500, Loss: 1.3657628297805786\nEpoch 126/500, Loss: 1.4678200483322144\nEpoch 127/500, Loss: 1.4671326875686646\nEpoch 128/500, Loss: 1.36150324344635\nEpoch 129/500, Loss: 1.2559449672698975\nEpoch 130/500, Loss: 1.4312748908996582\nEpoch 131/500, Loss: 1.347617506980896\nEpoch 132/500, Loss: 1.2590612173080444\nEpoch 133/500, Loss: 1.3558193445205688\nEpoch 134/500, Loss: 1.4326437711715698\nEpoch 135/500, Loss: 1.467429757118225\nEpoch 136/500, Loss: 1.314219355583191\nEpoch 137/500, Loss: 1.3990569114685059\nEpoch 138/500, Loss: 1.3383415937423706\nEpoch 139/500, Loss: 1.4632587432861328\nEpoch 140/500, Loss: 1.4023219347000122\nEpoch 141/500, Loss: 1.1631218194961548\nEpoch 142/500, Loss: 1.3379172086715698\nEpoch 143/500, Loss: 1.297218680381775\nEpoch 144/500, Loss: 1.3216474056243896\nEpoch 145/500, Loss: 1.434618353843689\nEpoch 146/500, Loss: 1.3579078912734985\nEpoch 147/500, Loss: 1.2898516654968262\nEpoch 148/500, Loss: 1.282408595085144\nEpoch 149/500, Loss: 1.4223246574401855\nEpoch 150/500, Loss: 1.3158806562423706\nEpoch 151/500, Loss: 1.1953128576278687\nEpoch 152/500, Loss: 1.2459877729415894\nEpoch 153/500, Loss: 1.2994874715805054\nEpoch 154/500, Loss: 1.302612543106079\nEpoch 155/500, Loss: 1.2675294876098633\nEpoch 156/500, Loss: 1.4400957822799683\nEpoch 157/500, Loss: 1.2305598258972168\nEpoch 158/500, Loss: 1.2739344835281372\nEpoch 159/500, Loss: 1.3675851821899414\nEpoch 160/500, Loss: 1.3628734350204468\nEpoch 161/500, Loss: 1.3691662549972534\nEpoch 162/500, Loss: 1.293811559677124\nEpoch 163/500, Loss: 1.3676642179489136\nEpoch 164/500, Loss: 1.407772421836853\nEpoch 165/500, Loss: 1.2280031442642212\nEpoch 166/500, Loss: 1.3920515775680542\nEpoch 167/500, Loss: 1.4240922927856445\nEpoch 168/500, Loss: 1.3694814443588257\nEpoch 169/500, Loss: 1.227104663848877\nEpoch 170/500, Loss: 1.3792213201522827\nEpoch 171/500, Loss: 1.3706021308898926\nEpoch 172/500, Loss: 1.4420329332351685\nEpoch 173/500, Loss: 1.3098291158676147\nEpoch 174/500, Loss: 1.2798041105270386\nEpoch 175/500, Loss: 1.3969556093215942\nEpoch 176/500, Loss: 1.346234917640686\nEpoch 177/500, Loss: 1.2229667901992798\nEpoch 178/500, Loss: 1.4496701955795288\nEpoch 179/500, Loss: 1.2361890077590942\nEpoch 180/500, Loss: 1.2736674547195435\nEpoch 181/500, Loss: 1.3782910108566284\nEpoch 182/500, Loss: 1.3447242975234985\nEpoch 183/500, Loss: 1.3045940399169922\nEpoch 184/500, Loss: 1.2089895009994507\nEpoch 185/500, Loss: 1.3905348777770996\nEpoch 186/500, Loss: 1.327528476715088\nEpoch 187/500, Loss: 1.3050599098205566\nEpoch 188/500, Loss: 1.3528259992599487\nEpoch 189/500, Loss: 1.3999656438827515\nEpoch 190/500, Loss: 1.2824718952178955\nEpoch 191/500, Loss: 1.3021794557571411\nEpoch 192/500, Loss: 1.3977004289627075\nEpoch 193/500, Loss: 1.4492133855819702\nEpoch 194/500, Loss: 1.3478622436523438\nEpoch 195/500, Loss: 1.366918683052063\nEpoch 196/500, Loss: 1.3854533433914185\nEpoch 197/500, Loss: 1.3295084238052368\nEpoch 198/500, Loss: 1.3942469358444214\nEpoch 199/500, Loss: 1.4321473836898804\nEpoch 200/500, Loss: 1.2979182004928589\nEpoch 201/500, Loss: 1.2410904169082642\nEpoch 202/500, Loss: 1.2906713485717773\nEpoch 203/500, Loss: 1.2752400636672974\nEpoch 204/500, Loss: 1.371184229850769\nEpoch 205/500, Loss: 1.3387643098831177\nEpoch 206/500, Loss: 1.355347752571106\nEpoch 207/500, Loss: 1.3241199254989624\nEpoch 208/500, Loss: 1.3125970363616943\nEpoch 209/500, Loss: 1.3207072019577026\nEpoch 210/500, Loss: 1.2831108570098877\nEpoch 211/500, Loss: 1.2456886768341064\nEpoch 212/500, Loss: 1.3154493570327759\nEpoch 213/500, Loss: 1.2910634279251099\nEpoch 214/500, Loss: 1.4221605062484741\nEpoch 215/500, Loss: 1.2259886264801025\nEpoch 216/500, Loss: 1.311069369316101\nEpoch 217/500, Loss: 1.2426769733428955\nEpoch 218/500, Loss: 1.402133584022522\nEpoch 219/500, Loss: 1.476737380027771\nEpoch 220/500, Loss: 1.4321094751358032\nEpoch 221/500, Loss: 1.3076975345611572\nEpoch 222/500, Loss: 1.4133800268173218\nEpoch 223/500, Loss: 1.3605185747146606\nEpoch 224/500, Loss: 1.405060887336731\nEpoch 225/500, Loss: 1.334851861000061\nEpoch 226/500, Loss: 1.2308028936386108\nEpoch 227/500, Loss: 1.3061658143997192\nEpoch 228/500, Loss: 1.3903756141662598\nEpoch 229/500, Loss: 1.3187154531478882\nEpoch 230/500, Loss: 1.368536353111267\nEpoch 231/500, Loss: 1.3917628526687622\nEpoch 232/500, Loss: 1.2278852462768555\nEpoch 233/500, Loss: 1.2846431732177734\nEpoch 234/500, Loss: 1.3515876531600952\nEpoch 235/500, Loss: 1.3065602779388428\nEpoch 236/500, Loss: 1.3344340324401855\nEpoch 237/500, Loss: 1.3630083799362183\nEpoch 238/500, Loss: 1.4821897745132446\nEpoch 239/500, Loss: 1.2985578775405884\nEpoch 240/500, Loss: 1.2997437715530396\nEpoch 241/500, Loss: 1.2841778993606567\nEpoch 242/500, Loss: 1.324097990989685\nEpoch 243/500, Loss: 1.4733511209487915\nEpoch 244/500, Loss: 1.3933162689208984\nEpoch 245/500, Loss: 1.364944577217102\nEpoch 246/500, Loss: 1.3979791402816772\nEpoch 247/500, Loss: 1.3644129037857056\nEpoch 248/500, Loss: 1.3637075424194336\nEpoch 249/500, Loss: 1.201255440711975\nEpoch 250/500, Loss: 1.329425573348999\nEpoch 251/500, Loss: 1.3288170099258423\nEpoch 252/500, Loss: 1.1919224262237549\nEpoch 253/500, Loss: 1.3144508600234985\nEpoch 254/500, Loss: 1.3863476514816284\nEpoch 255/500, Loss: 1.3295031785964966\nEpoch 256/500, Loss: 1.271275281906128\nEpoch 257/500, Loss: 1.3339003324508667\nEpoch 258/500, Loss: 1.2215908765792847\nEpoch 259/500, Loss: 1.2865666151046753\nEpoch 260/500, Loss: 1.3504053354263306\nEpoch 261/500, Loss: 1.360028862953186\nEpoch 262/500, Loss: 1.3717609643936157\nEpoch 263/500, Loss: 1.4853190183639526\nEpoch 264/500, Loss: 1.3602519035339355\nEpoch 265/500, Loss: 1.4732556343078613\nEpoch 266/500, Loss: 1.3177167177200317\nEpoch 267/500, Loss: 1.2659621238708496\nEpoch 268/500, Loss: 1.4701342582702637\nEpoch 269/500, Loss: 1.413631558418274\nEpoch 270/500, Loss: 1.5360215902328491\nEpoch 271/500, Loss: 1.3151555061340332\nEpoch 272/500, Loss: 1.41921865940094\nEpoch 273/500, Loss: 1.1768003702163696\nEpoch 274/500, Loss: 1.1887924671173096\nEpoch 275/500, Loss: 1.2650421857833862\nEpoch 276/500, Loss: 1.3729864358901978\nEpoch 277/500, Loss: 1.3468598127365112\nEpoch 278/500, Loss: 1.3221136331558228\nEpoch 279/500, Loss: 1.3734346628189087\nEpoch 280/500, Loss: 1.3379586935043335\nEpoch 281/500, Loss: 1.332195520401001\nEpoch 282/500, Loss: 1.3235629796981812\nEpoch 283/500, Loss: 1.4295663833618164\nEpoch 284/500, Loss: 1.2861453294754028\nEpoch 285/500, Loss: 1.370401382446289\nEpoch 286/500, Loss: 1.3431555032730103\nEpoch 287/500, Loss: 1.3684388399124146\nEpoch 288/500, Loss: 1.344014048576355\nEpoch 289/500, Loss: 1.4260939359664917\nEpoch 290/500, Loss: 1.313564419746399\nEpoch 291/500, Loss: 1.3187915086746216\nEpoch 292/500, Loss: 1.3334226608276367\nEpoch 293/500, Loss: 1.156938910484314\nEpoch 294/500, Loss: 1.407171368598938\nEpoch 295/500, Loss: 1.3898205757141113\nEpoch 296/500, Loss: 1.346934199333191\nEpoch 297/500, Loss: 1.2186812162399292\nEpoch 298/500, Loss: 1.3775558471679688\nEpoch 299/500, Loss: 1.2867122888565063\nEpoch 300/500, Loss: 1.2569700479507446\nEpoch 301/500, Loss: 1.3630595207214355\nEpoch 302/500, Loss: 1.4009108543395996\nEpoch 303/500, Loss: 1.3631902933120728\nEpoch 304/500, Loss: 1.279887318611145\nEpoch 305/500, Loss: 1.2792820930480957\nEpoch 306/500, Loss: 1.329405426979065\nEpoch 307/500, Loss: 1.290019154548645\nEpoch 308/500, Loss: 1.3465418815612793\nEpoch 309/500, Loss: 1.3832703828811646\nEpoch 310/500, Loss: 1.36493718624115\nEpoch 311/500, Loss: 1.2930041551589966\nEpoch 312/500, Loss: 1.2619683742523193\nEpoch 313/500, Loss: 1.382879614830017\nEpoch 314/500, Loss: 1.2661266326904297\nEpoch 315/500, Loss: 1.3219977617263794\nEpoch 316/500, Loss: 1.2772480249404907\nEpoch 317/500, Loss: 1.3980072736740112\nEpoch 318/500, Loss: 1.512447714805603\nEpoch 319/500, Loss: 1.3143521547317505\nEpoch 320/500, Loss: 1.407872200012207\nEpoch 321/500, Loss: 1.3203004598617554\nEpoch 322/500, Loss: 1.3370510339736938\nEpoch 323/500, Loss: 1.3469427824020386\nEpoch 324/500, Loss: 1.3876947164535522\nEpoch 325/500, Loss: 1.3084858655929565\nEpoch 326/500, Loss: 1.3623286485671997\nEpoch 327/500, Loss: 1.4255112409591675\nEpoch 328/500, Loss: 1.2801237106323242\nEpoch 329/500, Loss: 1.4290060997009277\nEpoch 330/500, Loss: 1.3029946088790894\nEpoch 331/500, Loss: 1.3722681999206543\nEpoch 332/500, Loss: 1.315942406654358\nEpoch 333/500, Loss: 1.3142396211624146\nEpoch 334/500, Loss: 1.326576590538025\nEpoch 335/500, Loss: 1.329392910003662\nEpoch 336/500, Loss: 1.3371820449829102\nEpoch 337/500, Loss: 1.3251127004623413\nEpoch 338/500, Loss: 1.3985623121261597\nEpoch 339/500, Loss: 1.3487945795059204\nEpoch 340/500, Loss: 1.3237086534500122\nEpoch 341/500, Loss: 1.4289737939834595\nEpoch 342/500, Loss: 1.3890104293823242\nEpoch 343/500, Loss: 1.330952763557434\nEpoch 344/500, Loss: 1.2352652549743652\nEpoch 345/500, Loss: 1.3822180032730103\nEpoch 346/500, Loss: 1.2270265817642212\nEpoch 347/500, Loss: 1.3184751272201538\nEpoch 348/500, Loss: 1.1818690299987793\nEpoch 349/500, Loss: 1.2479561567306519\nEpoch 350/500, Loss: 1.255104660987854\nEpoch 351/500, Loss: 1.392006278038025\nEpoch 352/500, Loss: 1.3894585371017456\nEpoch 353/500, Loss: 1.3359609842300415\nEpoch 354/500, Loss: 1.3882383108139038\nEpoch 355/500, Loss: 1.4565792083740234\nEpoch 356/500, Loss: 1.2568358182907104\nEpoch 357/500, Loss: 1.4181708097457886\nEpoch 358/500, Loss: 1.3707412481307983\nEpoch 359/500, Loss: 1.4259945154190063\nEpoch 360/500, Loss: 1.3682347536087036\nEpoch 361/500, Loss: 1.3702155351638794\nEpoch 362/500, Loss: 1.321228265762329\nEpoch 363/500, Loss: 1.2752448320388794\nEpoch 364/500, Loss: 1.4819782972335815\nEpoch 365/500, Loss: 1.3570326566696167\nEpoch 366/500, Loss: 1.3735246658325195\nEpoch 367/500, Loss: 1.3420805931091309\nEpoch 368/500, Loss: 1.3075265884399414\nEpoch 369/500, Loss: 1.3533836603164673\nEpoch 370/500, Loss: 1.2570019960403442\nEpoch 371/500, Loss: 1.364909291267395\nEpoch 372/500, Loss: 1.2658847570419312\nEpoch 373/500, Loss: 1.2943497896194458\nEpoch 374/500, Loss: 1.274057388305664\nEpoch 375/500, Loss: 1.2162716388702393\nEpoch 376/500, Loss: 1.2705482244491577\nEpoch 377/500, Loss: 1.4135202169418335\nEpoch 378/500, Loss: 1.3109321594238281\nEpoch 379/500, Loss: 1.2616267204284668\nEpoch 380/500, Loss: 1.257366418838501\nEpoch 381/500, Loss: 1.2202214002609253\nEpoch 382/500, Loss: 1.2933263778686523\nEpoch 383/500, Loss: 1.2674784660339355\nEpoch 384/500, Loss: 1.452089786529541\nEpoch 385/500, Loss: 1.3380866050720215\nEpoch 386/500, Loss: 1.4174290895462036\nEpoch 387/500, Loss: 1.3594897985458374\nEpoch 388/500, Loss: 1.4730232954025269\nEpoch 389/500, Loss: 1.2569254636764526\nEpoch 390/500, Loss: 1.3347293138504028\nEpoch 391/500, Loss: 1.4000133275985718\nEpoch 392/500, Loss: 1.195334553718567\nEpoch 393/500, Loss: 1.3710871934890747\nEpoch 394/500, Loss: 1.331115961074829\nEpoch 395/500, Loss: 1.2445260286331177\nEpoch 396/500, Loss: 1.251502275466919\nEpoch 397/500, Loss: 1.2470020055770874\nEpoch 398/500, Loss: 1.346673846244812\nEpoch 399/500, Loss: 1.2003388404846191\nEpoch 400/500, Loss: 1.364822506904602\nEpoch 401/500, Loss: 1.2118219137191772\nEpoch 402/500, Loss: 1.3724006414413452\nEpoch 403/500, Loss: 1.2605642080307007\nEpoch 404/500, Loss: 1.2728899717330933\nEpoch 405/500, Loss: 1.3061480522155762\nEpoch 406/500, Loss: 1.3480759859085083\nEpoch 407/500, Loss: 1.2265138626098633\nEpoch 408/500, Loss: 1.2816619873046875\nEpoch 409/500, Loss: 1.4062329530715942\nEpoch 410/500, Loss: 1.354245662689209\nEpoch 411/500, Loss: 1.3007975816726685\nEpoch 412/500, Loss: 1.3071800470352173\nEpoch 413/500, Loss: 1.3876334428787231\nEpoch 414/500, Loss: 1.3849140405654907\nEpoch 415/500, Loss: 1.392853856086731\nEpoch 416/500, Loss: 1.3048731088638306\nEpoch 417/500, Loss: 1.32475745677948\nEpoch 418/500, Loss: 1.3137927055358887\nEpoch 419/500, Loss: 1.3188573122024536\nEpoch 420/500, Loss: 1.1744121313095093\nEpoch 421/500, Loss: 1.3030105829238892\nEpoch 422/500, Loss: 1.4218945503234863\nEpoch 423/500, Loss: 1.2890836000442505\nEpoch 424/500, Loss: 1.3075417280197144\nEpoch 425/500, Loss: 1.3263499736785889\nEpoch 426/500, Loss: 1.2199147939682007\nEpoch 427/500, Loss: 1.3704744577407837\nEpoch 428/500, Loss: 1.4402599334716797\nEpoch 429/500, Loss: 1.3113373517990112\nEpoch 430/500, Loss: 1.3301879167556763\nEpoch 431/500, Loss: 1.3076093196868896\nEpoch 432/500, Loss: 1.2908581495285034\nEpoch 433/500, Loss: 1.1599767208099365\nEpoch 434/500, Loss: 1.3737977743148804\nEpoch 435/500, Loss: 1.2908296585083008\nEpoch 436/500, Loss: 1.365519642829895\nEpoch 437/500, Loss: 1.1809568405151367\nEpoch 438/500, Loss: 1.3033685684204102\nEpoch 439/500, Loss: 1.314029335975647\nEpoch 440/500, Loss: 1.2849743366241455\nEpoch 441/500, Loss: 1.271721363067627\nEpoch 442/500, Loss: 1.202500343322754\nEpoch 443/500, Loss: 1.3664436340332031\nEpoch 444/500, Loss: 1.290168285369873\nEpoch 445/500, Loss: 1.4808791875839233\nEpoch 446/500, Loss: 1.2814955711364746\nEpoch 447/500, Loss: 1.286141276359558\nEpoch 448/500, Loss: 1.2658940553665161\nEpoch 449/500, Loss: 1.1299327611923218\nEpoch 450/500, Loss: 1.4417184591293335\nEpoch 451/500, Loss: 1.3536123037338257\nEpoch 452/500, Loss: 1.2993723154067993\nEpoch 453/500, Loss: 1.4714239835739136\nEpoch 454/500, Loss: 1.3020297288894653\nEpoch 455/500, Loss: 1.3435670137405396\nEpoch 456/500, Loss: 1.2861295938491821\nEpoch 457/500, Loss: 1.2161961793899536\nEpoch 458/500, Loss: 1.395510196685791\nEpoch 459/500, Loss: 1.206942081451416\nEpoch 460/500, Loss: 1.3618844747543335\nEpoch 461/500, Loss: 1.2801764011383057\nEpoch 462/500, Loss: 1.2039378881454468\nEpoch 463/500, Loss: 1.1418429613113403\nEpoch 464/500, Loss: 1.1811600923538208\nEpoch 465/500, Loss: 1.3491911888122559\nEpoch 466/500, Loss: 1.2544900178909302\nEpoch 467/500, Loss: 1.4074560403823853\nEpoch 468/500, Loss: 1.2237783670425415\nEpoch 469/500, Loss: 1.2107266187667847\nEpoch 470/500, Loss: 1.292669415473938\nEpoch 471/500, Loss: 1.3730801343917847\nEpoch 472/500, Loss: 1.2793605327606201\nEpoch 473/500, Loss: 1.3463020324707031\nEpoch 474/500, Loss: 1.3812440633773804\nEpoch 475/500, Loss: 1.3245460987091064\nEpoch 476/500, Loss: 1.280930757522583\nEpoch 477/500, Loss: 1.3896713256835938\nEpoch 478/500, Loss: 1.36780846118927\nEpoch 479/500, Loss: 1.392830729484558\nEpoch 480/500, Loss: 1.3210169076919556\nEpoch 481/500, Loss: 1.2003475427627563\nEpoch 482/500, Loss: 1.3670920133590698\nEpoch 483/500, Loss: 1.2131462097167969\nEpoch 484/500, Loss: 1.2513338327407837\nEpoch 485/500, Loss: 1.3506444692611694\nEpoch 486/500, Loss: 1.3431682586669922\nEpoch 487/500, Loss: 1.119223713874817\nEpoch 488/500, Loss: 1.3180315494537354\nEpoch 489/500, Loss: 1.2419929504394531\nEpoch 490/500, Loss: 1.342767357826233\nEpoch 491/500, Loss: 1.3961352109909058\nEpoch 492/500, Loss: 1.2986057996749878\nEpoch 493/500, Loss: 1.3981599807739258\nEpoch 494/500, Loss: 1.397772192955017\nEpoch 495/500, Loss: 1.3186978101730347\nEpoch 496/500, Loss: 1.2917207479476929\nEpoch 497/500, Loss: 1.3067570924758911\nEpoch 498/500, Loss: 1.3452030420303345\nEpoch 499/500, Loss: 1.275528073310852\nEpoch 500/500, Loss: 1.4948883056640625\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n\n# Ensure the model is in evaluation mode\n\n# Initialize lists to collect predictions and labels for overall metrics\nall_y_pred_classes = []\nall_y_test_classes = []\ndbn.eval()\nwith torch.no_grad():\n    idx = 0\n    for X_batch, y_batch in test_loader:\n        # Move inputs and labels to the appropriate device\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n        \n        # Get predictions\n        y_pred = dbn(X_batch)\n        y_pred_classes = torch.argmax(y_pred, dim=1)\n        # print(y_pred_classes)\n        idx +=1\n        # if(idx <= 9):\n        #     print(y_pred)\n        # Calculate accuracy for this batch\n        accuracy = torch.sum(y_pred_classes == y_batch).item() / len(y_batch)\n        # print(f'Batch Accuracy: {accuracy * 100:.2f}%')\n        \n        # Collect predictions and true labels for overall metrics\n        all_y_pred_classes.extend(y_pred_classes.cpu().numpy())\n        all_y_test_classes.extend(y_batch.cpu().numpy())\naccuracy = accuracy_score(all_y_test_classes,all_y_pred_classes)\n# Calculate overall precision, recall, and F1-score\nprecision = precision_score(all_y_test_classes, all_y_pred_classes, average='weighted')\nrecall = recall_score(all_y_test_classes, all_y_pred_classes, average='weighted')\nf1 = f1_score(all_y_test_classes, all_y_pred_classes, average='weighted')\nprint(all_y_pred_classes)\nprint(f'Overall Accuracy: {accuracy:.2f}')\nprint(f'Overall Precision: {precision:.2f}')\nprint(f'Overall Recall: {recall:.2f}')\nprint(f'Overall F1 Score: {f1:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T08:28:34.287441Z","iopub.execute_input":"2024-12-06T08:28:34.288216Z","iopub.status.idle":"2024-12-06T08:28:34.303839Z","shell.execute_reply.started":"2024-12-06T08:28:34.288176Z","shell.execute_reply":"2024-12-06T08:28:34.302934Z"}},"outputs":[{"name":"stdout","text":"[0, 3, 3, 0, 3, 3, 1, 1, 2, 3, 3, 1, 0, 0, 3, 3, 1, 2, 2, 3, 3, 3, 3, 4, 0, 3, 1, 2, 3, 2, 2, 1, 3, 0, 2, 3, 2, 3, 3, 3, 1, 2, 2, 2, 0, 2, 0, 1, 3, 2, 0, 1, 3, 3, 2, 3, 3, 3, 0, 3, 0, 2, 4, 3, 0, 3, 1, 4, 2, 1, 3, 3, 2, 2, 4, 2, 2, 0, 3, 3, 3, 2, 4, 0, 1, 0, 2, 3, 1, 2, 3, 2, 2, 0, 3, 0, 2, 1, 3, 2, 2, 1, 3, 1, 0, 3, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 3, 4, 0, 0, 1, 3, 1, 3, 3, 3, 1, 3, 2, 1, 0, 3, 2, 2, 3, 3, 2, 0, 1, 3, 3, 0, 3, 3, 2, 3, 3, 1, 0, 1, 2, 3]\nOverall Accuracy: 0.53\nOverall Precision: 0.58\nOverall Recall: 0.53\nOverall F1 Score: 0.50\n","output_type":"stream"}],"execution_count":91},{"cell_type":"markdown","source":"## RBM + DBN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\nfrom tqdm import trange\n\nclass RBM:\n\n\tdef __init__(self, n_visible, n_hidden, lr=0.001, epochs=5, mode='bernoulli', batch_size=32, k=3, optimizer='adam', gpu=False, savefile=None, early_stopping_patience=5):\n\t\tself.mode = mode # bernoulli or gaussian RBM\n\t\tself.n_hidden = n_hidden #  Number of hidden nodes\n\t\tself.n_visible = n_visible # Number of visible nodes\n\t\tself.lr = lr # Learning rate for the CD algorithm\n\t\tself.epochs = epochs # Number of iterations to run the algorithm for\n\t\tself.batch_size = batch_size\n\t\tself.k = k\n\t\tself.optimizer = optimizer\n\t\tself.beta_1=0.9\n\t\tself.beta_2=0.999\n\t\tself.epsilon=1e-07\n\t\tself.m = [0, 0, 0]\n\t\tself.v = [0, 0, 0]\n\t\tself.m_batches = {0:[], 1:[], 2:[]}\n\t\tself.v_batches = {0:[], 1:[], 2:[]}\n\t\tself.savefile = savefile\n\t\tself.early_stopping_patience = early_stopping_patience\n\t\tself.stagnation = 0\n\t\tself.previous_loss_before_stagnation = 0\n\t\tself.progress = []\n\n\t\tif torch.cuda.is_available() and gpu==True:  \n\t\t\tdev = \"cuda:0\" \n\t\telse:  \n\t\t\tdev = \"cpu\"  \n\t\tself.device = torch.device(dev)\n\n\t\t# Initialize weights and biases\n\t\tstd = 4 * np.sqrt(6. / (self.n_visible + self.n_hidden))\n\t\tself.W = torch.normal(mean=0, std=std, size=(self.n_hidden, self.n_visible))\n\t\tself.vb = torch.zeros(size=(1, self.n_visible), dtype=torch.float32)\n\t\tself.hb = torch.zeros(size=(1, self.n_hidden), dtype=torch.float32)\n\n\t\tself.W = self.W.to(self.device)\n\t\tself.vb = self.vb.to(self.device)\n\t\tself.hb = self.hb.to(self.device)\n\t\t\n\tdef sample_h(self, x):\n\t\twx = torch.mm(x, self.W.t())\n\t\tactivation = wx + self.hb\n\t\tp_h_given_v = torch.sigmoid(activation)\n\t\tif self.mode == 'bernoulli':\n\t\t\treturn p_h_given_v, torch.bernoulli(p_h_given_v)\n\t\telse:\n\t\t\treturn p_h_given_v, torch.add(p_h_given_v, torch.normal(mean=0, std=1, size=p_h_given_v.shape))\n\n\tdef sample_v(self, y):\n\t\twy = torch.mm(y, self.W)\n\t\tactivation = wy + self.vb\n\t\tp_v_given_h =torch.sigmoid(activation)\n\t\tif self.mode == 'bernoulli':\n\t\t\treturn p_v_given_h, torch.bernoulli(p_v_given_h)\n\t\telse:\n\t\t\treturn p_v_given_h, torch.add(p_v_given_h, torch.normal(mean=0, std=1, size=p_v_given_h.shape))\n\t\n\tdef adam(self, g, epoch, index):\n\t\tself.m[index] = self.beta_1 * self.m[index] + (1 - self.beta_1) * g\n\t\tself.v[index] = self.beta_2 * self.v[index] + (1 - self.beta_2) * torch.pow(g, 2)\n\n\t\tm_hat = self.m[index] / (1 - np.power(self.beta_1, epoch)) + (1 - self.beta_1) * g / (1 - np.power(self.beta_1, epoch))\n\t\tv_hat = self.v[index] / (1 - np.power(self.beta_2, epoch))\n\t\treturn m_hat / (torch.sqrt(v_hat) + self.epsilon)\n\n\tdef update(self, v0, vk, ph0, phk, epoch):\n\t\tdW = (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()\n\t\tdvb = torch.sum((v0 - vk), 0)\n\t\tdhb = torch.sum((ph0 - phk), 0)\n\n\t\tif self.optimizer == 'adam':\n\t\t\tdW = self.adam(dW, epoch, 0)\n\t\t\tdvb = self.adam(dvb, epoch, 1)\n\t\t\tdhb = self.adam(dhb, epoch, 2)\n\n\t\tself.W += self.lr * dW\n\t\tself.vb += self.lr * dvb\n\t\tself.hb += self.lr * dhb\n\n\tdef train(self, dataset):\n\t\tdataset = dataset.to(self.device)\n\t\tlearning = trange(self.epochs, desc=str('Starting...'))\n\t\tfor epoch in learning:\n\t\t\ttrain_loss = 0\n\t\t\tcounter = 0\n\t\t\tfor batch_start_index in range(0, dataset.shape[0]-self.batch_size, self.batch_size):\n\t\t\t\tvk = dataset[batch_start_index:batch_start_index+self.batch_size]\n\t\t\t\tv0 = dataset[batch_start_index:batch_start_index+self.batch_size]\n\t\t\t\tph0, _ = self.sample_h(v0)\n\n\t\t\t\tfor k in range(self.k):\n\t\t\t\t\t_, hk = self.sample_h(vk)\n\t\t\t\t\t_, vk = self.sample_v(hk)\n\t\t\t\tphk, _ = self.sample_h(vk)\n\t\t\t\tself.update(v0, vk, ph0, phk, epoch+1)\n\t\t\t\ttrain_loss += torch.mean(torch.abs(v0-vk))\n\t\t\t\tcounter += 1\n\t\t\t\n\t\t\tself.progress.append(train_loss.item()/counter)\n\t\t\tdetails = {'epoch': epoch+1, 'loss': round(train_loss.item()/counter, 4)}\n\t\t\tlearning.set_description(str(details))\n\t\t\tlearning.refresh()\n\t\t\t\n\t\t\tif train_loss.item()/counter > self.previous_loss_before_stagnation and epoch>self.early_stopping_patience+1:\n\t\t\t\tself.stagnation += 1\n\t\t\t\tif self.stagnation == self.early_stopping_patience-1:\n\t\t\t\t\tlearning.close()\n\t\t\t\t\tprint(\"Not Improving the stopping training loop.\")\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tself.previous_loss_before_stagnation = train_loss.item()/counter\n\t\t\t\tself.stagnation = 0\n\t\tlearning.close()\n\t\tif self.savefile is not None:\n\t\t\tmodel = {'W':self.W, 'vb':self.vb, 'hb':self.hb}\n\t\t\ttorch.save(model, self.savefile)\n\n\tdef load_rbm(self, savefile):\n\t\tloaded = torch.load(savefile)\n\t\tself.W = loaded['W']\n\t\tself.vb = loaded['vb']\n\t\tself.hb = loaded['hb']\n\n\t\tself.W = self.W.to(self.device)\n\t\tself.vb = self.vb.to(self.device)\n\t\tself.hb = self.hb.to(self.device)\n\n\n\ndef trial_dataset():\n\tdataset = []\n\tfor _ in range(1000):\n\t\tt = []\n\t\tfor _ in range(10):\n\t\t\tif random.random()>0.75:\n\t\t\t\tt.append(0)\n\t\t\telse:\n\t\t\t\tt.append(1)\n\t\tdataset.append(t)\n\n\tfor _ in range(1000):\n\t\tt = []\n\t\tfor _ in range(10):\n\t\t\tif random.random()>0.75:\n\t\t\t\tt.append(1)\n\t\t\telse:\n\t\t\t\tt.append(0)\n\t\tdataset.append(t)\n\n\tdataset = np.array(dataset, dtype=np.float32)\n\tnp.random.shuffle(dataset)\n\tdataset = torch.from_numpy(dataset)\n\treturn dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:32:17.365026Z","iopub.execute_input":"2024-12-06T09:32:17.365299Z","iopub.status.idle":"2024-12-06T09:32:17.387138Z","shell.execute_reply.started":"2024-12-06T09:32:17.365274Z","shell.execute_reply":"2024-12-06T09:32:17.386260Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\nfrom tqdm import trange\n\nclass DBN:\n\tdef __init__(self, input_size, layers, mode='bernoulli', gpu=False, k=5, savefile=None):\n\t\tself.layers = layers\n\t\tself.input_size = input_size\n\t\tself.layer_parameters = [{'W':None, 'hb':None, 'vb':None} for _ in range(len(layers))]\n\t\tself.k = k\n\t\tself.mode = mode\n\t\tself.savefile = savefile\n\n\tdef sample_v(self, y, W, vb):\n\t\twy = torch.mm(y, W)\n\t\tactivation = wy + vb\n\t\tp_v_given_h =torch.sigmoid(activation)\n\t\tif self.mode == 'bernoulli':\n\t\t\treturn p_v_given_h, torch.bernoulli(p_v_given_h)\n\t\telse:\n\t\t\treturn p_v_given_h, torch.add(p_v_given_h, torch.normal(mean=0, std=1, size=p_v_given_h.shape))\n\n\tdef sample_h(self, x, W, hb):\n\t\twx = torch.mm(x, W.t())\n\t\tactivation = wx + hb\n\t\tp_h_given_v = torch.sigmoid(activation)\n\t\tif self.mode == 'bernoulli':\n\t\t\treturn p_h_given_v, torch.bernoulli(p_h_given_v)\n\t\telse:\n\t\t\treturn p_h_given_v, torch.add(p_h_given_v, torch.normal(mean=0, std=1, size=p_h_given_v.shape))\n\n\tdef generate_input_for_layer(self, index, x):\n\t\tif index>0:\n\t\t\tx_gen = []\n\t\t\tfor _ in range(self.k):\n\t\t\t\tx_dash = x.clone()\n\t\t\t\tfor i in range(index):\n\t\t\t\t\t_, x_dash = self.sample_h(x_dash, self.layer_parameters[i]['W'], self.layer_parameters[i]['hb'])\n\t\t\t\tx_gen.append(x_dash)\n\n\t\t\tx_dash = torch.stack(x_gen)\n\t\t\tx_dash = torch.mean(x_dash, dim=0)\n\t\telse:\n\t\t\tx_dash = x.clone()\n\t\treturn x_dash\n\n\tdef train_DBN(self, x):\n\t\tfor index, layer in enumerate(self.layers):\n\t\t\tif index==0:\n\t\t\t\tvn = self.input_size\n\t\t\telse:\n\t\t\t\tvn = self.layers[index-1]\n\t\t\thn = self.layers[index]\n\n\t\t\trbm = RBM(vn, hn, epochs=100, mode='bernoulli', lr=0.0005, k=10, batch_size=128, gpu=True, optimizer='adam', early_stopping_patience=10)\n\t\t\tx_dash = self.generate_input_for_layer(index, x)\n\t\t\trbm.train(x_dash)\n\t\t\tself.layer_parameters[index]['W'] = rbm.W.cpu()\n\t\t\tself.layer_parameters[index]['hb'] = rbm.hb.cpu()\n\t\t\tself.layer_parameters[index]['vb'] = rbm.vb.cpu()\n\t\t\tprint(\"Finished Training Layer:\", index, \"to\", index+1)\n\t\tif self.savefile is not None:\n\t\t\ttorch.save(self.layer_parameters, self.savefile)\n\n\tdef reconstructor(self, x):\n\t\tx_gen = []\n\t\tfor _ in range(self.k):\n\t\t\tx_dash = x.clone()\n\t\t\tfor i in range(len(self.layer_parameters)):\n\t\t\t\t_, x_dash = self.sample_h(x_dash, self.layer_parameters[i]['W'], self.layer_parameters[i]['hb'])\n\t\t\tx_gen.append(x_dash)\n\t\tx_dash = torch.stack(x_gen)\n\t\tx_dash = torch.mean(x_dash, dim=0)\n\n\t\ty = x_dash\n\n\t\ty_gen = []\n\t\tfor _ in range(self.k):\n\t\t\ty_dash = y.clone()\n\t\t\tfor i in range(len(self.layer_parameters)):\n\t\t\t\ti = len(self.layer_parameters)-1-i\n\t\t\t\t_, y_dash = self.sample_v(y_dash, self.layer_parameters[i]['W'], self.layer_parameters[i]['vb'])\n\t\t\ty_gen.append(y_dash)\n\t\ty_dash = torch.stack(y_gen)\n\t\ty_dash = torch.mean(y_dash, dim=0)\n\n\t\treturn y_dash, x_dash\n\n\tdef initialize_model(self):\n\t\tprint(\"The Last layer will not be activated. The rest are activated using the Sigoid Function\")\n\t\tmodules = []\n\t\tfor index, layer in enumerate(self.layer_parameters):\n\t\t\tmodules.append(torch.nn.Linear(layer['W'].shape[1], layer['W'].shape[0]))\n\t\t\tif index < len(self.layer_parameters) - 1:\n\t\t\t\tmodules.append(torch.nn.Softmax())\n\t\tmodel = torch.nn.Sequential(*modules)\n\n\t\tfor layer_no, layer in enumerate(model):\n\t\t\tif layer_no//2 == len(self.layer_parameters)-1:\n\t\t\t\tbreak\n\t\t\tif layer_no%2 == 0:\n\t\t\t\tmodel[layer_no].weight = torch.nn.Parameter(self.layer_parameters[layer_no//2]['W'])\n\t\t\t\tmodel[layer_no].bias = torch.nn.Parameter(self.layer_parameters[layer_no//2]['hb'])\n\n\t\treturn model\n\ndef trial_dataset():\n\tdataset = []\n\tfor _ in range(1000):\n\t\tt = []\n\t\tfor _ in range(10):\n\t\t\tif random.random()>0.75:\n\t\t\t\tt.append(0)\n\t\t\telse:\n\t\t\t\tt.append(1)\n\t\tdataset.append(t)\n\n\tfor _ in range(1000):\n\t\tt = []\n\t\tfor _ in range(10):\n\t\t\tif random.random()>0.75:\n\t\t\t\tt.append(1)\n\t\t\telse:\n\t\t\t\tt.append(0)\n\t\tdataset.append(t)\n\n\tdataset = np.array(dataset, dtype=np.float32)\n\tnp.random.shuffle(dataset)\n\tdataset = torch.from_numpy(dataset)\n\treturn dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:32:17.389013Z","iopub.execute_input":"2024-12-06T09:32:17.389281Z","iopub.status.idle":"2024-12-06T09:32:17.407781Z","shell.execute_reply.started":"2024-12-06T09:32:17.389256Z","shell.execute_reply":"2024-12-06T09:32:17.406850Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Create dataset\n# X, y = generate_labeled_dataset()\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize DBN\ndbn = DBN(input_size=X_train_tensor.shape[1], layers=[72, 2*72-1,2*72-1], mode='none', k=5)\ndbn.train_DBN(X_train_tensor)\n\n# Create a classification model\nmodel = dbn.initialize_model()\n# Append final classification layer\n# model.add_module(\"second_final_layer\", nn.Linear(dbn.layers[-1], 5))\nmodel.add_module(\"final_layer\", nn.Linear(dbn.layers[-1], 5))  # 5 classes\n# model.add_module(\"softmax\", nn.Softmax(dim=1))\n\n# Train the classifier\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\naccuracies_selfdetect= []\n# Training loop\nnum_epochs = 801\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n\n    if(epoch%25 == 0):\n        with torch.no_grad():\n            model.eval()\n            y_pred = model(X_val_tensor)\n            y_pred = F.softmax(y_pred,dim=1)\n            y_pred_classes = y_pred.argmax(dim=1)\n            accuracy = accuracy_score(y_val_tensor.numpy(), y_pred_classes.numpy())\n            accuracies_selfdetect.append(accuracy)\n        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}, Accuracy:{accuracy:.4f} \")\n\n# Evaluate the model\nepochs_ = len(accuracies_selfdetect)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(6, 6))\nplt.plot( range(0, epochs_), accuracies_selfdetect, marker='o', label='Self-detection model')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Validation Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:32:17.408821Z","iopub.execute_input":"2024-12-06T09:32:17.409097Z","iopub.status.idle":"2024-12-06T09:32:40.311228Z","shell.execute_reply.started":"2024-12-06T09:32:17.409073Z","shell.execute_reply":"2024-12-06T09:32:40.310155Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"{'epoch': 100, 'loss': 0.7408}: 100%|██████████| 100/100 [00:06<00:00, 16.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Finished Training Layer: 0 to 1\n","output_type":"stream"},{"name":"stderr","text":"{'epoch': 60, 'loss': 0.4219}:  58%|█████▊    | 58/100 [00:03<00:02, 16.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Not Improving the stopping training loop.\nFinished Training Layer: 1 to 2\n","output_type":"stream"},{"name":"stderr","text":"{'epoch': 53, 'loss': 0.5085}:  52%|█████▏    | 52/100 [00:03<00:02, 16.38it/s]\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Not Improving the stopping training loop.\nFinished Training Layer: 2 to 3\nThe Last layer will not be activated. The rest are activated using the Sigoid Function\nEpoch 0/801, Loss: 1.6094181537628174, Accuracy:0.1842 \nEpoch 25/801, Loss: 1.3942610025405884, Accuracy:0.3596 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50/801, Loss: 1.2055351734161377, Accuracy:0.3947 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75/801, Loss: 1.069076657295227, Accuracy:0.5044 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100/801, Loss: 0.9433290958404541, Accuracy:0.5746 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 125/801, Loss: 0.8414238095283508, Accuracy:0.6228 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 150/801, Loss: 0.7599010467529297, Accuracy:0.6404 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 175/801, Loss: 0.6954249739646912, Accuracy:0.6711 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 200/801, Loss: 0.6205833554267883, Accuracy:0.7412 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 225/801, Loss: 0.5777430534362793, Accuracy:0.7500 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 250/801, Loss: 0.5269625782966614, Accuracy:0.7500 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 275/801, Loss: 0.4990319311618805, Accuracy:0.7500 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 300/801, Loss: 0.4672379493713379, Accuracy:0.7544 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 325/801, Loss: 0.4627358317375183, Accuracy:0.7281 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 350/801, Loss: 0.45875343680381775, Accuracy:0.7544 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 375/801, Loss: 0.40246355533599854, Accuracy:0.7544 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 400/801, Loss: 0.4199000597000122, Accuracy:0.7719 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 425/801, Loss: 0.3690565228462219, Accuracy:0.7544 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 450/801, Loss: 0.35670173168182373, Accuracy:0.7719 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 475/801, Loss: 0.32691940665245056, Accuracy:0.7763 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 500/801, Loss: 0.3339668810367584, Accuracy:0.7456 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 525/801, Loss: 0.304362028837204, Accuracy:0.7807 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 550/801, Loss: 0.2849087715148926, Accuracy:0.7675 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 575/801, Loss: 0.28670045733451843, Accuracy:0.7412 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 600/801, Loss: 0.2737092971801758, Accuracy:0.7763 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 625/801, Loss: 0.2569817304611206, Accuracy:0.7500 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 650/801, Loss: 0.2584320306777954, Accuracy:0.7588 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 675/801, Loss: 0.2517814636230469, Accuracy:0.7368 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 700/801, Loss: 0.22224462032318115, Accuracy:0.7719 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 725/801, Loss: 0.21005713939666748, Accuracy:0.7588 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 750/801, Loss: 0.21303527057170868, Accuracy:0.7632 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 775/801, Loss: 0.21582263708114624, Accuracy:0.7939 \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 800/801, Loss: 0.20254148542881012, Accuracy:0.7588 \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    y_pred = model(X_test_tensor)\n    y_pred = F.softmax(y_pred,dim=1)\n    y_pred_classes = y_pred.argmax(dim=1)\naccuracy = accuracy_score(y_test_tensor.numpy(), y_pred_classes.numpy())\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:30:53.444891Z","iopub.execute_input":"2024-12-06T09:30:53.445550Z","iopub.status.idle":"2024-12-06T09:30:53.453212Z","shell.execute_reply.started":"2024-12-06T09:30:53.445508Z","shell.execute_reply":"2024-12-06T09:30:53.452260Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 59.39%\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"f1 = f1_score(y_test_tensor.numpy(), y_pred_classes.numpy(), average='weighted')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:15:38.361471Z","iopub.execute_input":"2024-12-06T09:15:38.361859Z","iopub.status.idle":"2024-12-06T09:15:38.370331Z","shell.execute_reply.started":"2024-12-06T09:15:38.361811Z","shell.execute_reply":"2024-12-06T09:15:38.369393Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"report = classification_report(y_test_tensor.numpy(), y_pred_classes.numpy(), digits=3)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T09:16:03.326323Z","iopub.execute_input":"2024-12-06T09:16:03.326682Z","iopub.status.idle":"2024-12-06T09:16:03.339695Z","shell.execute_reply.started":"2024-12-06T09:16:03.326641Z","shell.execute_reply":"2024-12-06T09:16:03.338742Z"}},"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0      0.766     0.900     0.828        40\n           1      0.660     0.733     0.695        45\n           2      0.711     0.593     0.646        54\n           3      0.897     0.795     0.843        44\n           4      0.688     0.717     0.702        46\n\n    accuracy                          0.738       229\n   macro avg      0.744     0.748     0.743       229\nweighted avg      0.742     0.738     0.737       229\n\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"rnn_data = [\n    (0, 0.1974), (25, 0.3158), (50, 0.5307), (75, 0.6184), (100, 0.6623),\n    (125, 0.7105), (150, 0.7763), (175, 0.7807), (200, 0.7851), (225, 0.7939),\n    (250, 0.7939), (275, 0.8289), (300, 0.8333), (325, 0.8246), (350, 0.8246),\n    (375, 0.8421), (400, 0.8421), (425, 0.8553), (450, 0.8421), (475, 0.8596),\n    (500, 0.8640), (525, 0.8509), (550, 0.8509), (575, 0.8640), (600, 0.8640),\n    (625, 0.8333), (650, 0.8114), (675, 0.8465), (700, 0.8596), (725, 0.8509),\n    (750, 0.8333), (775, 0.8289), (800, 0.8289)\n]\n\ngru_data = [\n    (0, 0.2018), (25, 0.5044), (50, 0.8070), (75, 0.8289), (100, 0.8333),\n    (125, 0.8553), (150, 0.8553), (175, 0.8860), (200, 0.8816), (225, 0.8728),\n    (250, 0.8860), (275, 0.8816), (300, 0.8904), (325, 0.8860), (350, 0.8904),\n    (375, 0.8860), (400, 0.8816), (425, 0.8816), (450, 0.8816), (475, 0.8860),\n    (500, 0.8991), (525, 0.8772), (550, 0.8947), (575, 0.8860), (600, 0.9035),\n    (625, 0.9123), (650, 0.9035), (675, 0.9035), (700, 0.9079), (725, 0.9079),\n    (750, 0.9123), (775, 0.9123), (800, 0.9123)\n]\n\nlstm_data = [\n    (0, 0.1842), (25, 0.3860), (50, 0.8728), (75, 0.9342), (100, 0.9518),\n    (125, 0.9342), (150, 0.9298), (175, 0.9254), (200, 0.9561), (225, 0.9649),\n    (250, 0.9649), (275, 0.9649), (300, 0.9649), (325, 0.9649), (350, 0.9649),\n    (375, 0.9649), (400, 0.9649), (425, 0.9649), (450, 0.9649), (475, 0.9605),\n    (500, 0.9605), (525, 0.9605), (550, 0.9605), (575, 0.9561), (600, 0.9561),\n    (625, 0.9561), (650, 0.9518), (675, 0.9518), (700, 0.9518), (725, 0.9518),\n    (750, 0.9518), (775, 0.9518), (800, 0.9518)\n]\n\npaper_model = [\n    (0, 0.1491), (25, 0.3553), (50, 0.3289), (75, 0.4912), (100, 0.5482),\n    (125, 0.5921), (150, 0.6623), (175, 0.6754), (200, 0.6798), (225, 0.7018),\n    (250, 0.7149), (275, 0.7368), (300, 0.7412), (325, 0.7500), (350, 0.7149),\n    (375, 0.6754), (400, 0.7456), (425, 0.7675), (450, 0.7719), (475, 0.7807),\n    (500, 0.7500), (525, 0.7982), (550, 0.7763), (575, 0.7982), (600, 0.7939),\n    (625, 0.7895), (650, 0.8026), (675, 0.7982), (700, 0.7851), (725, 0.7982),\n    (750, 0.7895), (775, 0.7939), (800, 0.8026)\n]\n\ndef unpack_data(data):\n    return zip(*data)\n\nrnn_x, rnn_y = unpack_data(rnn_data)\ngru_x, gru_y = unpack_data(gru_data)\nlstm_x, lstm_y = unpack_data(lstm_data)\npaper_x, paper_y = unpack_data(paper_model)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(rnn_x, rnn_y, label='RNN', marker='o')\nplt.plot(gru_x, gru_y, label='GRU', marker='s')\nplt.plot(lstm_x, lstm_y, label='LSTM', marker='^')\nplt.plot(paper_x, paper_y, label='DBN', marker='d')\n\nplt.title('Model Performance Comparison')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T11:02:09.637416Z","iopub.execute_input":"2024-12-12T11:02:09.637847Z","iopub.status.idle":"2024-12-12T11:02:09.976999Z","shell.execute_reply.started":"2024-12-12T11:02:09.637810Z","shell.execute_reply":"2024-12-12T11:02:09.975611Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m paper_x, paper_y \u001b[38;5;241m=\u001b[39m unpack_data(paper_model)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     51\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(rnn_x, rnn_y, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(gru_x, gru_y, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m'\u001b[39m, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}